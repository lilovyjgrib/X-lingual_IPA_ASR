# -*- coding: utf-8 -*-
"""model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1shBat_ZO7e75J4i3LzVe85NNcVlfVz3C
"""

"""
PyTorch conversion of the TensorFlow/Keras ResNet-BiRNN ASR model
Original Author (TF version): Manish Dhakal (2022)
Publication: https://doi.org/10.1109/ICICT54344.2022.9850832
Converted by: Haejin Cho
Convresion Date: 2025.8.9.
"""


import torch
import torch.nn as nn
import torch.nn.functional as F
from configs import model_name, input_dim, num_classes


class ResBlock(nn.Module):
    def __init__(self, num_cnn_layers, cnn_filters, cnn_kernel_size, use_resnet=True):
        super(ResBlock, self).__init__()
        self.use_resnet = use_resnet
        layers = []
        for _ in range(num_cnn_layers):
            layers.append(nn.Conv1d(cnn_filters, cnn_filters, cnn_kernel_size, padding=cnn_kernel_size // 2))
            layers.append(nn.BatchNorm1d(cnn_filters))
            layers.append(nn.PReLU())
        self.res_block = nn.Sequential(*layers)

    def forward(self, x):
        res = self.res_block(x)
        if self.use_resnet:
            return x + res
        return res


class ASRModel(nn.Module):
    def __init__(self, ip_channel, num_classes, num_res_blocks=3, num_cnn_layers=1, cnn_filters=50,
                 cnn_kernel_size=15, num_rnn_layers=2, rnn_dim=170, num_dense_layers=1,
                 dense_dim=300, use_birnn=True, use_resnet=True, rnn_type="lstm", rnn_dropout=0.15):
        super(ASRModel, self).__init__()

        # Initial Conv layer
        self.init_conv = nn.Sequential(
            nn.Conv1d(ip_channel, cnn_filters, cnn_kernel_size, padding=cnn_kernel_size // 2),
            nn.BatchNorm1d(cnn_filters),
            nn.PReLU()
        )

        # Residual blocks
        self.res_blocks = nn.Sequential(
            *[ResBlock(num_cnn_layers, cnn_filters, cnn_kernel_size, use_resnet) for _ in range(num_res_blocks)]
        )

        # RNN layers
        rnn_module = nn.GRU if rnn_type.lower() == "gru" else nn.LSTM
        rnn_input_dim = cnn_filters
        self.rnns = nn.ModuleList()
        for _ in range(num_rnn_layers):
            if use_birnn:
                self.rnns.append(rnn_module(rnn_input_dim, rnn_dim, batch_first=True, dropout=rnn_dropout, bidirectional=True))
                rnn_input_dim = rnn_dim * 2
            else:
                self.rnns.append(rnn_module(rnn_input_dim, rnn_dim, batch_first=True, dropout=rnn_dropout))
                rnn_input_dim = rnn_dim

        # Dense layers
        dense_layers = []
        dense_in_dim = rnn_input_dim
        for _ in range(num_dense_layers):
            dense_layers.append(nn.Linear(dense_in_dim, dense_dim))
            dense_layers.append(nn.ReLU())
            dense_in_dim = dense_dim
        self.dense_layers = nn.Sequential(*dense_layers)

        # Output layer
        self.out_layer = nn.Linear(dense_in_dim, num_classes)

    def forward(self, x):
        # x: (batch, time, features) -> (batch, features, time) for Conv1d
        x = x.transpose(1, 2)
        x = self.init_conv(x)
        x = self.res_blocks(x)

        # Back to (batch, time, features) for RNN
        x = x.transpose(1, 2)
        for rnn in self.rnns:
            x, _ = rnn(x)

        x = self.dense_layers(x)
        x = self.out_layer(x)
        x = F.softmax(x, dim=-1)
        return x

if __name__ == '__main__':
  input_dim = 39
  num_classes = 47
  model_name = 'ResNet_pytorch'

  model = ASRModel(
      ip_channel=input_dim,
      num_classes=num_classes,
      num_res_blocks=5,
      num_cnn_layers=2,
      cnn_filters=50,
      cnn_kernel_size=15,
      rnn_dim=170,
      num_rnn_layers=2,
      num_dense_layers=1,
      dense_dim=340,
      use_birnn=True,
      rnn_type='lstm',
  )

  x = torch.rand(2, 100, input_dim) # (B, T, F)
  y = model(x)
  print(y.shape)

# Data
# processing
def read_TIMIT(path):
  '''
  args:
    path: path of TIMIT data(mfcc features, phoneme labels)
  return:
    feats: list of list for each audio samples
    labels: list of list for each audio samples
  '''

  feats, labels= [], []
  length_feats, length_labels= [], []

  # read processed TIMIT data
  # list of dictionarys with keys being 'mfcc', 'phonemes', 'path'
  samples= torch.load(path, weights_only= False)
  for idx in range(len(samples)):
    feats.append(samples[idx]['mfcc'])
    labels.append([phoneme.strip() for phoneme in samples[idx]['phonemes']])
  return feats, labels

def process_label(labels):
  # (1) remove h#, pau, epi
  labels = [[symbol for symbol in label if symbol not in 'h# epi pau'] for label in labels]

  # (2) handle diphthongs
  diphthongs= ['ey', 'aw', 'ay', 'ow']
  # ey 'bait' -> split
  # aw 'bout' -> split
  # ay 'bite' -> split
  # oy 'boy' -> oh + y
  # ow 'boat' -> split

  diphthong_regex= re.compile('|'.join(sorted(map(re.escape, diphthongs),
                                              key= len, reverse= True)))
  oy_regex= re.compile('oy')

  for i in range(len(labels)):
    label = ' '.join(labels[i])
    label = diphthong_regex.sub(lambda x: ' '.join(x.group()), label)
    label = oy_regex.sub('oh y', label).split()

    labels[i] = label

  # (3) handle similari sounds (merge)

  merge_ipa= {
    # marginal sounds
    'ax-h': 'ax',
    'bcl': 'b',
    'dcl': 'd',
    'gcl': 'g',
    'kcl': 'k',
    'pcl': 'p',
    'tcl': 't',

    'en': 'n',
    'em': 'm',
    'el': 'l',
    'eng': 'ng',

    ## /ɹ/ sound
    # 'axr': 'r' ? 'ɹ' ?
    # 'dx': 'r',
    # 'nx': 'r',
    # 'er': 'r', 'ɹ' ?

    # /h/ sound
    'hh': 'h',
    }

    labels_merge = [[merge_ipa.get(symbol, symbol) for symbol in label] for label in labels]

    # (3) handling duplicate sounds after merge
    labels_final = []
    for label, label_merge in zip(labels, labels_merge):
      label = ' '.join(label)
      label_merge = ' '.join(label_merge)
      for key, item in merge_ipa.items():
        if ' '.join([key, item]) in label:
          label_merge = re.sub(' '.join([item, item]), item, label_merge)

      labels_final.append(label_merge.split())

    return labels_final


def create_IPAdictionary(labels):
  '''
  args:
    labels: list of list containing sequence of label for each audio sample
  return: ipa2idx
        dictionary of IPA_label to index
  '''
  ipas= set()
  for label in labels:
    ipas= ipas.union(set(label))
  ipas= sorted(ipas)

  ipa2idx= {ipa:(idx+2) for idx, ipa in enumerate(ipas)}
  ipa2idx['<blank>']= 0

  return ipa2idx

if __name__ == '__main__':
  path= r'timit_mfcc_data.pt'
  feats, labels= read_TIMIT(path)


  # check mfcc feature matrix dimension
  print(f'MFCC feature matrix Shape (one audio sample):\t{feats[-1].shape}')
  # check IPA repository
  print(f'phoneme labels(one audio sample):\t{labels[-1]}')
  ## seems like 'h#' marks sos and eos

  print('\n'.join([f'num_frames:\t{len(feats[0])}', f'num_labels:\t{len(labels[0])}']))

  # process labels
  labels = process_labels(labels)

  # split train / dev trainset
  train_feats, dev_feats, train_labels, dev_labels = train_test_split(feats, labels)

  # create IPA dictionary
  ipa2idx = create_IPAdictionary(labels)
  print(*sorted(ipa2idx))

  print(f'the number of IPA labels in TIMIT:\t {len(ipa2idx)}')
  print(f'Index of blank symbol <blank>":\t {ipa2idx["<blank>"]}')

"""#### Dataset"""

# Data
# dataset
class PhonemeASRDataset(Dataset):
  def __init__(self, feats, labels, ipa2idx):
    super(PhonemeASRDataset, self).__init__()
    self.feats, self.labels= feats, labels
    self.ipa2idx= ipa2idx

  def __len__(self):
    return len(self.feats)

  def __getitem__(self, idx):
      feat, label= self.feats[idx], self.labels[idx]
      label= [ipa2idx[ipa] for ipa in label]

      return torch.tensor(feat), torch.tensor(label, dtype= torch.long)


def pad_collate(batch, pad_value_feat= 0, pad_value_label= 0):
    '''
      for collate_fn in DataLoader function

    args:
      batch: a list of tuples (mfcc, label)
      return: padded_mfccs, padded_labels, input_lengths, target_lengths
    '''

    mfccs, labels= zip(*batch)

    # find max length for mfcc(time step) and label in the current batch
    max_len_feats= max(mfcc.shape[0] for mfcc in mfccs)
    max_len_labels= max(label.shape[0] for label in labels)

    # pad mfcc matrices and labels
    padded_mfccs= [F.pad(mfcc, (0, 0, 0, max_len_feats - mfcc.shape[0]), value= pad_value_feat) for mfcc in mfccs]
    padded_labels= [F.pad(label, (0, max_len_labels - label.shape[0]), value= pad_value_label) for label in labels]

    # calculate lengths of input and target lengths
    input_lengths = torch.tensor([mfcc.shape[0] for mfcc in padded_mfccs], dtype = torch.long)
    target_lengths = torch.tensor([label.shape[0] for label in padded_labels], dtype = torch.long)


    # Stack the padded tensors
    padded_mfccs= torch.stack(padded_mfccs)
    padded_labels= torch.stack(padded_labels)

    return padded_mfccs, padded_labels, input_lengths, target_lengths

if __name__ == '__main__':
  train_feats, dev_feats, train_labels, dev_labels = train_test_split(feats, labels)


  train_ds= PhonemeASRDataset(train_feats, train_labels, ipa2idx= ipa2idx)
  train_loader= DataLoader(train_ds, batch_size= 64,
                          shuffle= True, collate_fn= pad_collate) # outputs (B, T, C)