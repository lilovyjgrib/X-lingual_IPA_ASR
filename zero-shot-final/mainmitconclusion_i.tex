% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\pagestyle{plain}
\usepackage{fontspec}
\usepackage{tipa} % IPA package
\setmainfont{Times New Roman}
\newfontfamily\ipafont{Charis SIL}
\newcommand{\ipa}[1]{{\ipafont #1}}
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\usepackage{array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
% Remove the "review" option to generate the final version.
\usepackage[final]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets


% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{natbib}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Zero-Shot Cross-lingual Phoneme Recognition from Yoruba to English}

\author{Aaron Bahr, Nikita L. Beklemishev, Haejin Cho, Kai Seidenspinner \and Ilinca Vandici \\
         Universtität Tübingen }

\begin{document}
\pdfoutput=1
\maketitle
\outauthor
\begin{abstract}
In this paper, we pre-train an acoustic phoneme recognition model on the TIMIT dataset and evaluate its performance on the Yoruba portion of the Common voice data. We make use of the CTC architecture, allowing us to forego the need for time-aligned input data, and examine the model’s performance after transfer through a thorough and linguistically motivated feature weighting metric, reaching a 0.36 average Phoneme Error Rate on the Yoruba set. Leveraging the predictions produced by our model, we take an in-depth look at the effects of learning and transfer. All codes are available in our \href{https://github.com/lilovyjgrib/X-lingual_IPA_ASR}{Github}.
\end{abstract}

\section{Introduction}
Efficiently training an ASR system requires a rich, ideally time-aligned dataset. For low-resource languages, despite documentation efforts, exploiting the properties of transfer learning by pre-training on  high-resource language remains a sensible option. For the purpose of zero-shot evaluation, picking a set of languages with similar phoneme inventories remains the practice yielding the best performance. We chose to work on transferring American English (< West Germanic < Indo-European) to Yoruba (< Volta-Niger < Atlantic-Congo), primarily spoken in Nigeria, whose phoneme inventories overlap to a large extent, despite different areas and \textit{phyla}.
We focus solely on producing a consistent, generalized phonemic transcription, conditioned only on acoustic segments. 
In terms of the model architecture, using the \textit{Connectionist Temporal Classification Loss} ensured that we could evaluate on non time-aligned data, with the additional benefit of enhancing adaptability to context (add reference maybe?).
Along with obtaining good performance on both languages, we aim to propose an efficient evaluation metric by employing a linguistically sound feature-weighted version of PER. We attempt to disentangle training from transfer errors by presenting a holistic view of our results, analyzing the confusion patterns in our predictions through metrics like cross-entropy and normalized PMI.
In recent years, multilingual ASR has gained more and more attention. Models like (REFERENCE) perform well on low resource languages, given the diversity and size of the training dataset which allows them to form representations for a variety of phonemes. However, as models grow in size, interpretability becomes a more delicate task. Acknowledging this, we choose to focus on one language pair instead allows us to gain a deeper insight of the mechanisms at work during transfer.


\subsection{Background}

Transfer learning occurs when using a model to carry out a task different from the one it has originally been trained on. This can be done in different ways, for instance by using the pre-defined representations a model has formed on downstream tasks, or by fine-tuning the model weights in order to skew the distribution towards that of the new task \cite{yadav2022survey} . This last variant is particularly popular when training multilingual ASR models, which due to compute progress, have becoming more common REFERE,CE. 
In our case, we examine zero-shot transfer, by running inference directly on the new task, without the intermediary of pre-defined feature mappings. 

It is commonly understood that the more similar the probability distribution under the new task is to that of the original task, the higher the likelihood of successful transfer learning. For the case at hand, this can be assumed to mean that good performance on the task will likely be dependent on how similar the phones in each language pair are. 
In the case of phoneme recognition, we are trying to approximate a function mapping an acoustic signal to a transcription. This raises the question of the codomain of this function, which will need to at least partly overlap across both tasks in order to obtain an interpretable output. Orthographic characters do not fulfill this condition, since what they represent is subjective in each language. 

Luckily, for linguistics, the relevant representation of speech is phonemes, which tend to correspond to sound types---which are, by and large, commonly shared. Out of all the sounds that the vocal tract can produce, languages use only a small subset, and some sounds are much more common than others. \citep[p.~6]{hayes2009} The shared knowledge of sound types can be described using IPA symbols. \cite{ladefoged2011} The IPA presents a well-defined framework through which this similarity is defined, and enables us to unify transcriptions across languages, ensuring that the learned function maps from the acoustic domain to a common representation. Furthermore, by analyzing \textbf{confusion and correspondence} patterns of the function from acoustics to phonemes inherited from English, and applied to Yoruba, we shed light on the variation and universals in these phonological systems. 
																																																						

\subsection{English to Yoruba transfer}
The phonemic systems of Yoruba and English are not entirely similar., as  can be observed in Appendix [ref], cf. \cite{Adesola2024, AhiaEtAl2024, moran2014}. This is further complicated by several factors that might influence downstream performance. (1) Above all, the IPA does not aim to fully describe language's phonemic system, in a structuralist sence, but to represent ``universal'' phonetic features. \citep[p. ~40]{vanderhulst2017}. Thus, for instance, we expect that the vowel space of both languages will have a different partition. (2) Perceptually, one sound might need to be decomposed into two in context of another phonological system. Yoruba has double-articulated sounds: \ipa{/k͡p}, \ipa{/ĩ}, which could be mapped to sequences like \ipa{/kp/} or \ipa{/in/}). (3) Phonological and phonotatic constraints are less universal \cite{maddieson2010} than inventories, which creates different sound sequence distributions. For example, English frequent diphthongs make the vowel--glide sequences much more probable than they are in Yoruba. It, in turn, has constraints on (C)V(N) syllables and vowel harmony. \cite{Przezdziecki2005}. (4) Ideally, the model should learn the contrasts of Yoruba, but it in fact is trained for distinctions of English. For example, unlike English, \ipa{[n \textasciitilde l]}, \ipa{[ɾ \textasciitilde ɹ]} are allophonic in Yoruba. The shallow English transcription distinguishes allophones like \ipa{[ɨ \textasciitilde i]}, and such cases will noise the correspondence between the predictions and Yoruba labels.

\subsection{The inventories problem} Given the differences discussed above, relating English IPA predictions to the Yoruba IPA gold standard becomes a problem: What is a matching sequence, given that the targets only partly overlap? Phonological feature theory \cite{chomsky1968} provides us with a theoretical framework within which ``similarity'' exists on a gradient scale.
Each sound has a feature representation indicating which natural classes it belongs and does not belong to. Although sounds vary in how many features from the overall set they specify, it is convenient to represent each sound as a vector over the entire feature inventory, with each element coded as positive (+1), negative (−1), or unspecified (0). The Hamming distance between two such vectors, restricted to non-zero dimensions and normalized by dimensionality, then serves as the dissimilarity measure (see Appendix C[ref]). This approach has been used for a long time in fields like cognate detection[] and dialectometry.\cite{nerbonne2010} . Among many notable databanks aggregating the knowledge of features, such as PHOIBLE \cite{moran2014} and SoundVector \cite{rubehn2024}, we chose PanPhon \cite{mortensen2016} for its balance between the quality of representation and availability. The details of the implementation are discussed in Inference[ref].

\section{Dataset}
\paragraph{Training set} For training we use the TIMIT corpus. \citet{garofolo1993timit} includes 6300 utterances recorded by 630 speakers of 8 major English dialects across the US. Annotations were done according to the customized IPA convention based on ARPAbet \cite{cmudict} (please refer to Appendix \ref{sec:appendix}). As both English and Yoruba are pluricentric languages, training the model on a variety of dialects will likely improve the performance in downstream tasks. Even though TIMIT confines itself to the 1980s US language roof, it represents the existing dialectal variation well. This phonetic variation is necessary for learning to generalize over the variation in sounds, which is particularly relevant for transfer learning. Another advantage of TIMIT is its ubiquity in ASR studies, which gives us confidence in yielding baseline results, comparable with other works in the area.

\paragraph{Preprocessing the training set} For usage, we first concatenated the train, validation, and test splits before again randomly dividing it into train and validation splits (with a 75 to 25 ratio). We perform a Fast Fourier Transform on the audio data, collecting 39 log-scale MFCC features, including first and second-order derivatives.

The TIMIT alphabet contained 63 unique labels in total. In order reduce prediction complexity and ensure the compatibility of the phonemic representation between both languages (and to make way for the IPA mapping process), we merged or split several labels. This included allophones not annotated in the Yoruba corpus: \ipa{<ax-h> /ə̥/} and \ipa{/ə/}, syllabic sonorants, e.g. \ipa{<eng> /ŋ̍/} and \ipa{/ŋ/}. Closures \ipa{<dcl> /d̚ /} and the following releases \ipa{/d/} were joined into one label: we did not expect systematic unreleased closures in \textbf{open-syllable} Yoruba. \cite{Adesola2024} In the end, 15 label types were merged. As for the splitting the combinatorially large inventory of English diphthongs, we split them into vowel--glide sequences, keeping the vowels from the IPA convention: thus \ipa{<oy> /ɔɪ/} became \ipa{<ao y> /ɔ j/}. This step was necessary since Yoruba does not have diphthongs. \citet{Przezdziecki2005} We also concluded that splitting them will not perplex the prediction given that the CTC decoding does not need time alignment, and our evaluation ignores word boundaries.

\paragraph{Evaluation set} The Yoruba section of Common Voice was used as a test dataset. Common Voice is a multilingual crowd-sourced corpus aimed for Speech Recognition purposes. \cite{Ardila2020} The audios were recorded by certified native speakers of each language. Annotations are suggested and later validated by other native speaker users via votes.There are 3.4k samples in total, with each sample including a MP3 file, speaker ID and audio transcription written in Yoruba orthography. The dataset also includes data from different dialects. To an extent, this accounts for the performance gaps that are shown to arise between standard Yoruba and other dialects in NLP tasks \citet{AhiaEtAl2024}.

\paragraph{Processing the evaluation set} The sets of train (1.4k), validation (913) and test (1.1k) were again concatenated and used together for testing. We kept the original threshold of downvotes for invalidated samples. As for the audio data, features were extracted under the same parameters as for the TIMIT dataset.
Since Common Voice only provides an orthographic representation of the sentence, it was necessary to implement a grapheme-to-phoneme pipeline in order to obtain an IPA representation that could then be compared with the model output. For this purpose, we used \texttt{Epitran} Python module \cite{epitran}, and pre-processed the resulting strings. Word boundaries and pauses were removed. We also ignored the tone annotation ({\ipa{˧}, \ipa{˦}, \ipa{˨}), although seeing the transfer abilities could be an interesting branch of research. We have removed the marginally phonemic \ipa{/ɔ̃}, \ipa{ŋ/} from the inventory, merging them with their allophones \ipa{/ã/} and \ipa{/n/}. \cite{AllenPulleyblankAjiboye2013} Another marginal \ipa{/ɛ̃/} remained. The data also turned out to contain one occurrence of dialectal \ipa{<ụ> /ʊ/} \cite{Przezdziecki2005}, which is too small to generalize from, so we removed it as well from the evaluation for clarity. The resulting Yoruba IPA inventory is available in Appendix \ref{sec:appendixb}.

\section{Model and Traning}
\subsection{ResNet-Bi-LSTM model}
Our model is based on that of \cite{dhakal2022automatic}, which used ResNet-BiLSTM model for Nepali Speech Recognition. The reason why we chose this model as our base reference was (1) to use pure neural-network-based model so that it is relatively easy to train and light-weight in terms of memory, (2) to include residual connection which is largely used in transformers as well as deep neural network models, and (3) to explore whether a model that is trained from scratch can also perform well on monolingual zero-shot cross-lingual speech recognition task unlike \cite{xu2021}. We will first briefly overall architecture and setting of the original model and then list our adjustments.

The model starts with initial CNN layer and 5 consecutive ResNet blocks. These make up the 'ResNet Encoder', which aims to capture local dependencies. Each block consists of 2 unit blocks, with each unit block containing an initial convolution, Batch Normalization, using PrELU in lieu of the activation function. Each kernel has its size parameter set to 15, and performs 50 maps. The key idea behind residual connections is that adding the original input at deeper stages in the forward pass will result in grounding the representations \cite{ravanelli2019speakerrecognitionrawwaveform} have attested that residual connections noticeably stabilize and optimize deep neural network training, which \cite{dhakal2022automatic} confirms extends to audio data. A Bi-LSTM encoder part then follows the Residual Encoder. Bi-LSTM is designed to reflect distinctive temporally-related features in two opposite directions. \cite{dhakal2022automatic} has two RNN layers with its dimension being both 170. As final layers, two dense layers and ReLU activation takes the output of bi-LSTM and projects the 170 dimensional output onto the phonemic embedding space.


We have made several crucial changes based on multiple experiments in order to prevent overfitting and to restrict the depth of the model under zero-shot cross-lingual task setting. (1) We downsize the number of ResNetEncoder layers from 5 to 3, and change the number of unit blocks from 2 to 1, meaning that we reduced the original model to a third of its depth.  All other ResNet encoder settings remain fixed as in\cite{dhakal2022automatic}.
We made changes to the dimensions of the Bi-LSTM encoder (reducing the hidden dimension of the RNN from 170 to 128) and the dense layer (from 340 to 256).
We also diminished hidden dimensions of Bi-LSTM encoder and dense layer,  From 170 to 128 as to hidden dimension of RNN layer and from 340 to 256 as to dense layer dimension. Finally, we introduced a stronger dropout rate to the dense layer, with the aim of facilitating cross-lingual application. Refer to the following visualization that marks key differences between \cite{dhakal2022automatic} and our model.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{modelarchitecture+.png}
    \caption{Left: model architecture of \cite{dhakal2022automatic} , Right: Adjusted model architecture for English-Yoruba cross-lingual phoneme recognition. Changes highlighted with a red circle.}
    \label{fig:architecture}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{residualblock.PNG}
    \caption{Left: residual block of \cite{dhakal2022automatic}, Right: Adjusted residual block for English-Yoruba cross-lingual phoneme recognition. Changes highlighted with a red circle.}
    \label{fig:residualblock}
\end{figure}

We employed the Connectionist Temporal Classification (CTC) loss as our training criterion. Unlike conventional frame-level cross-entropy, CTC does not require pre-aligned input-label pairs, which makes it especially suitable for low-resource languages such as Yoruba, where alignment information is unavailable. The key idea of CTC is to introduce a blank symbol and permit label repetitions so that multiple frame-level alignments can correspond to the same output sequence. During training, CTC computes the negative log-likelihood of the target sequence from the probabilities of all valid alignments:
$$L_{CTC} = \sum_{(X, Y) \in D} - \log P_{CTC}(Y \mid X)$$
where the probability of a target sequence $Y$ given the input $X$ is defined as
\[
\begin{aligned}
P_{CTC}(Y \mid X) &= \sum_{A \in B^{-1}(Y)} P(A \mid X) \\
                  &= \sum_{A \in B^{-1}(Y)} \prod_{t=1}^{T} p(a_t \mid h_t)
\end{aligned}
\]
with $A=(a_1, \dots, a_T)$ being a frame-level alignment, $B$ the collapse function that removes blanks and repeated labels, and $h_t$ the hidden representation at time $t$. The blank symbol $\epsilon$ needs to be manually added to the alphabet so that the model can output blanks during training and inference.

At decoding time, the CTC collapse operation removes duplicates and blanks to produce the final prediction sequence $\hat{Y}$.
$$\hat{Y} = \arg\max_{Y} P_{CTC}(Y \mid X).$$

It is worth mentioning that, along with the purely practical lack of an aligned corpus, CTC can also in some cases present an advantage over architectures that rely on alignment. Indeed, \citet{hannun2017} argues that aligned data might induce some biases, especially if the corpus is not diverse enough in regards to the kind of speech contexts available.

\subsection{Train}
Train settings were set through multiple experiments. The number of epochs is 50 and batch size was 64. We have adopted Adam as our stochastic gradient optimizer and set weight decay as 1e-4. A plateau-based learning rate scheduler was used, with an initial learning rate was 1e-3. The total number of parameters are 0.8 million. With the (hyper)parameter settings given above, training was noticeably stable and robust to overfitting. Both training and validation loss and PER consistently decreased and we halted training at the 16th epoch where Train PER reached 0.0223 and Validation PER reached a similar figure, 0.0339.

\section{Results}

As seen from the final PER results, the model shows a promising performance on the English dataset. We now introduce the framework we used for evaluation, before delving deeper into the results produced by transfer.

\subsection{Evaluation Method} 
We evaluate the output sequence with relation to the corresponding gold labels sequence, by computing its Weighted Phoneme Error Rate. This measure accounts for different sequence length (with insertion and deletion costs that can be adjusted, much like in the Levenshtein algorithm) and includes a feature-based substitution cost, based on the phonological information vectors we built using PanPhon. Some of the distinctions included by the library are not relevant in describing the English and Yoruba inventories:  [\texttt{sg}, \texttt{velaric}, \texttt{long}, \texttt{hitone}, \texttt{hireg}]. We built on and adapted the original PanPhone source code to suit our goals, and also offer an optimized version of the alignment calculation algorithm. The main assumption behind our evaluation scheme is that the alignments achieved with this method are not only minimally costly, but also intuitively correct. As we see in Discussion, it is not always the case.

\subsection{Inference}

During inference, we observed that our model had a tendency to over-generate, making the output consistently longer than the gold labels. A look at the Appendix [ref] reveals that this is a transfer defect of the model. Some of the worst performances of the model arise from this over-generation. Whenever this happens, the inserted sounds tend to be somewhat rare phones: a lot of \ipa{/ʔ, ɨ, ʉ, k, t, n/}. This mildly leads to hypothesis that it reflects some extra-linguistic noise that was not present in the training TIMIT recordings. Indeed, we noticed that the Common Voice audio data often included pauses, especially preceding and following the utterance. While for future investigations, it might be relevant to include noise reduction techniques, we chose to tackle this issue at the decoding/ evaluation step, by lowering the deletion cost to \texttt{0.5} and insertion cost to \texttt{0.75}.

\paragraph{Results} Default PER achieved on Yoruba was \texttt{0.34}. It ranged from \texttt{0.18} to \texttt{1.11}, with median \texttt{0.30}. The best and worst predictions, as well as the alignment of the median prediction are in Appendix [ref]. With the original Levenshtein costs for deletion and insertion, median PER increases to \texttt{0.35}.

\subsection{Performance by phoneme}

Alignment counts \(weighted Levenshtein\) are depicted below, in the form of a confusion matrix \ref{fig:postconf}. Color encodes the posterior probabilities of the prediction labels given the gold labels.

\begin{figure}
    \label{fig:postconf}
    \centering
    \includegraphics[width=1\linewidth]{EngYorConfusionMatrix.png}
    \caption{English-Yoruba Confusion Matrix with Posterior Probability}
\end{figure}

\paragraph{Confusion entropy} We used conditional entropy to find which Yoruba phonemes are transferrable (Table \ref{table:confusionent}), assuming that a well-transferred sound will have a less uniform distribution over predicted sounds: Most alignments would belong to a couple of phonetically close classes. Most of the formulas we used were drawn from \citep{Bentz2022}.
As shown by the results (ref to below or sum), it is evident that phonetically similar sounds in terms of feature distance were the most common alignments for more accurately predicted phonemes. 
For less accurately predicted phonemes the most common alignments are the phonetically similar sounds, in terms of feature distance: e.g /s/. It seems, front vowels and consonants articulated in the front were broadly predicted correctly, whereas back vowels and consonants were more frequently confused. We found no possible phonetic explanation to this other than that it could be an artifact of the feature system.

\begin{table}[ht]
\centering
\caption{Confusion entropy, empirical and theoretical (feature-based) correspondences.}
\label{table:confusionent}
\begin{tabular}{c l l c}
\toprule
\textbf{Gold} & \textbf{Confusions} & \textbf{Similar} & \textbf{H} \\
\midrule
\ipa{s} & \ipa{[z, s, n, v]} & \ipa{[s, z, t, ʃ]} & 3.40 \\
\ipa{m} & \ipa{[m, n, l, w]} & \ipa{[m, b, n, p]} & 3.67 \\
\ipa{d͡ʒ} & \ipa{[d͡ʒ, d, n, z]} & \ipa{[d͡ʒ, t͡ʃ, ʒ, ʃ]} & 3.69 \\
\ipa{b} & \ipa{[b, v, m, n]} & \ipa{[b, p, m, v]} & 3.76 \\
\ipa{ʃ} & \ipa{[ʒ, ʃ, z, h]} & \ipa{[ʃ, ʒ, s, t͡ʃ]} & 3.80 \\
\ipa{i} & \ipa{[i, ɨ, ɪ, j]} & \ipa{[i, e, ɨ, ɪ]} & 3.81 \\
\ipa{w} & \ipa{[w, j, ʔ, u]} & \ipa{[w, ʍ, u, ə˞]} & 3.82 \\
\ipa{e} & \ipa{[i, ɪ, ɨ, j]} & \ipa{[e, i, æ, ɛ]} & 3.87 \\
\ipa{j} & \ipa{[j, i, w, ʔ]} & \ipa{[j, i, ɪ, w]} & 3.92 \\
\ipa{ĩ} & \ipa{[i, ɪ, ɨ, u]} & \ipa{[i, e, ɨ, ɪ]} & 3.93 \\
\ipa{ɛ̃} & \ipa{[ɪ, ɛ, n, ʔ]} & \ipa{[ɛ, e, ə, ɪ]} & 3.99 \\
\ipa{d} & \ipa{[d, n, ɡ, b]} & \ipa{[d, t, n, z]} & 4.00 \\
\ipa{n} & \ipa{[n, m, l, d]} & \ipa{[n, d, l, m]} & 4.03 \\
\ipa{r} & \ipa{[ɹ, n, j, l]} & \ipa{[r, l, d, n]} & 4.07 \\
\ipa{ɛ} & \ipa{[ɪ, i, ɛ, ɨ]} & \ipa{[ɛ, e, ə, ɪ]} & 4.09 \\
\ipa{k} & \ipa{[ɡ, ʔ, k, h]} & \ipa{[k, ɡ, h, ŋ]} & 4.10 \\
\ipa{ɡ͡b} & \ipa{[b, m, w, v]} & \ipa{[ɡ, b, k, p]} & 4.13 \\
\ipa{f} & \ipa{[f, v, z, h]} & \ipa{[f, v, p, s]} & 4.14 \\
\ipa{u} & \ipa{[u, ɨ, i, ʉ]} & \ipa{[u, ʉ, o, ɨ]} & 4.16 \\
\ipa{t} & \ipa{[z, d, n, t]} & \ipa{[t, d, s, θ]} & 4.16 \\
\ipa{k͡p} & \ipa{[b, w, ʔ, v]} & \ipa{[k, p, ɡ, b]} & 4.18 \\
\ipa{ɡ} & \ipa{[ɡ, b, w, v]} & \ipa{[ɡ, k, ŋ, b]} & 4.18 \\
\ipa{ũ} & \ipa{[u, ɨ, w, ɪ]} & \ipa{[u, ʉ, o, ɨ]} & 4.21 \\
\ipa{l} & \ipa{[l, n, ɹ, j]} & \ipa{[l, d, n, z]} & 4.26 \\
\ipa{h} & \ipa{[ʔ, h, w, l]} & \ipa{[h, k, ʔ, j]} & 4.33 \\
\ipa{o} & \ipa{[u, ɨ, o, i]} & \ipa{[o, ɔ, ʌ, a]} & 4.41 \\
\ipa{a} & \ipa{[a, ʌ, æ, ɑ]} & \ipa{[a, æ, ɑ, ʌ]} & 4.52 \\
\ipa{ɔ} & \ipa{[o, ə, ʌ, ɨ]} & \ipa{[ɔ, o, ə, ʊ]} & 4.53 \\
\ipa{ã} & \ipa{[ɨ, o, u, ə]} & \ipa{[a, æ, ɑ, ʌ]} & 4.55 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Mapping entropy} Another interesting question  is whether we can restore the Yoruba phonemes reliably from the TIMIT-based phoneme predictions. To see how unambiguous this mapping is and how much information our model contains after transfer, we use the conditional entropy of the golden label probabilities given the predicted label. Appendix [ref]. The overall entropy was \texttt{3.5} bits, i.e. about \texttt{11} label targets per prediction. However, for some of the labels it was much smaller, thus signifying almost one-to-one mapping from predicted labels to Yoruba.\ref{table:entfreq} The number would have likely been even smaller if not for deletions. Another thing raising the expected uncertainty is that the sounds that can map to multiple Yoruba labels, e.g. \ipa{/m, l, w/}, are among the most frequent ones.\ref{fig:predphonefreq}.

All of the back vowels, whose high confusions we mentioned above, are now among the most certainly transferrable. This means that the high confusion entropy observed actually stems from the sheer amount of possible vowel segments in English. When English partitions the vowel space to \ipa{/a, ɑ, æ, ʌ/}, they all are included in Yoruba \ipa{/a/}. Ambiguous mappings (indicated by high entropy) also occur for sounds absent in Yoruba: IPA \ipa{/ʔ, ɹ, ə˞, ð, θ/} have no clear correspondence in Yoruba to gravitate to. This mostly applies to consonants, which coincides with a commonly stated \cite{hayes2009, ladefoged2011} intuition that vowel realizations distribute continuously in the acoustic space, while consonant allophones belong to one of some discrete targets.

\begin{table}[ht]
\centering
\caption{Mapping entropy and frequentmost correspondences.}
\label{table:entfreq}
\begin{tabular}{c l c @{\hskip 1.5em} c l c}
\toprule
\textbf{Gold} & \textbf{Corr} & \textbf{H} & \textbf{Gold} & \textbf{Corr} & \textbf{H} \\
\midrule
\ipa{a}  & \ipa{[a, ã]} & 1.96 & \ipa{ɨ} & \ipa{[i, a]} & 3.41 \\
\ipa{ɑ}  & \ipa{[a, ã]} & 2.01 & \ipa{ʉ} & \ipa{[u, ũ]} & 3.41 \\
\ipa{æ}  & \ipa{[a, e]} & 2.32 & \ipa{ɪ} & \ipa{[ɪ, i]} & 3.48 \\
\ipa{ʃ}  & \ipa{[ʃ, s]} & 2.33 & \ipa{ə˞} & \ipa{[ʊ, ɔ]} & 3.52 \\
\ipa{ʌ}  & \ipa{[a, e]} & 2.49 & \ipa{n} & \ipa{[n, d]} & 3.59 \\
\ipa{t͡ʃ} & \ipa{[d͡ʒ, ʃ]} & 2.63 & \ipa{z} & \ipa{[s, d]} & 3.63 \\
\ipa{d͡ʒ} & \ipa{[d͡ʒ, ʃ]} & 2.80 & \ipa{ɹ} & \ipa{[j, l]} & 3.66 \\
\ipa{i}  & \ipa{[i, e]} & 2.82 & \ipa{t} & \ipa{[t, d]} & 3.67 \\
\ipa{o}  & \ipa{[o, ɔ]} & 3.05 & \ipa{θ} & \ipa{[s, t]} & 3.69 \\
\ipa{ʊ}  & \ipa{[ʊ, ɔ]} & 3.06 & \ipa{ʔ} & \ipa{[h, k]} & 3.73 \\
\ipa{ɔ}  & \ipa{[ɔ, o]} & 3.06 & \ipa{w} & \ipa{[w, u]} & 3.75 \\
\ipa{ɛ}  & \ipa{[ɛ, e]} & 3.07 & \ipa{m} & \ipa{[m, b]} & 3.78 \\
\ipa{k}  & \ipa{[k, ɡ]} & 3.09 & \ipa{ŋ} & \ipa{[ŋ, ɡ]} & 3.80 \\
\ipa{ʒ}  & \ipa{[ʃ, d͡ʒ]} & 3.10 & \ipa{d} & \ipa{[d, t]} & 3.82 \\
\ipa{ə}  & \ipa{[ɔ, ɛ]} & 3.28 & \ipa{l} & \ipa{[l, d]} & 3.91 \\
\ipa{ɜ˞} & \ipa{[ʊ, ɔ]} & 3.28 & \ipa{b} & \ipa{[b, m]} & 3.91 \\
\ipa{s}  & \ipa{[s, t]} & 3.28 & \ipa{j} & \ipa{[j, i]} & 3.95 \\
\ipa{e}  & \ipa{[e, i]} & 3.31 & \ipa{ɡ} & \ipa{[ɡ, k]} & 4.05 \\
\ipa{u}  & \ipa{[u, ũ]} & 3.32 & \ipa{ð} & \ipa{[d, l]} & 4.16 \\
\ipa{f}  & \ipa{[f, s]} & 3.33 & \ipa{h} & \ipa{[h, k]} & 4.25 \\
\ipa{p}  & \ipa{[b, f]} & 3.36 & \ipa{v} & \ipa{[f, b]} & 4.26 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Phone embeddings space}

After having fully trained the model, we can take a look into the representations it has formed for each item of the vocabulary by collecting phone embeddings from the last layer. We attempt to plot these phone vectors in an easily interpretable manner, in order to investigate what the internal model representations for each language might look like. Ideally, the partition in the space would reflect similarities and differences for each target language, possibly even extending to reflect a scale of articulatory features. We run inference to produce phone embeddings from the last layer output (log probabilities in the phoneme classes space) and plot these using dimensionality reduction. 

\paragraph{Phone posteriogram embeddings} 
Note that to obtain the representations below, we only selected items that were correctly predicted, after CTC collapse. By extending the extraction to all the time frames in one example (including frames where the highest probability lies on the blank token), we can obtain a spectrogram-like visualization of the shifting of probabilities over time. It runs against the intuition we have from spectrograms, but we consider it a suitable representation. This allows to see precisely how we arrived at a predicted string.~\ref{fig:ppg}. As a product of CTC decoding, posteriograms appear to be different from spectrograms. Since the model is allowed to predict blank symbols, it tends to allocate blank symbols for majority of frames, while producing occasional spikes for non-blank phonemes. These spikes often appear at the time steps where phoneme predictions first become feasible.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ppg20.png}
    \caption{Phone posteriograms for one Yoruba sentence, after softmax, first 14 classes}
    \label{fig:ppg}
\end{figure}

\paragraph{Extraction}  We collected the PPGs for each correctly predicted phoneme (that is deemed a match by our alignment algorithm). Due to the over-generation problem, it is entirely possible that some matching predictions are skipped in the process, but since we have not yet established a way to decode the noise that is generated, we limit ourselves to accurately aligned pairs. It also constraints us from analyzing the non-corresponding phonemes, like \ipa{/ũ/} and \ipa{/k͡p/}. 

We employ \textit{UMAP} to obtain 2-dimensional embeddings from vocabulary sized, 46-dimensional vector, for which we obtain the mean by phoneme. We settled on UMAP since it maintains the non-linear assumption of t-SNE regarding the data while being less dependent on initialization.  While both plots seem to group similar sounds together to an extent, plotting inference on both datasets remains tricky due to 1) the different probability distributions and the difference in dataset sizes 2) the lack of evidence that UMAP will systematically plots distances between phonemes. We can see that in some cases, similar sounds do tend to cluster, but the extent to which these patterns can be identified different in both datasets.

\paragraph{Yoruba phones} In the reduced Yoruba space \ref{fig:yoremb}, cardinal vowels cluster in the same space in the graph, with their distance mostly corresponding to their feature weighted equivalent. We see how natural classes of phonemes group together in space: the sibilants \ipa{/s/} and \ipa{/ʃ/}, plosives, sonorants, palatals. In that arrangement, the \textit{y}-axis clearly corresponds to sonority hierarchy[quote]. It can also be argued that the \textit{x}-axis loosely represents a gradual backness scale, when including acoustically related parameters like lip protrusion, in the case of labials.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{yorlast.png}
    \caption{Yoruba Phone Embeddings (with English correspondences only)}
    \label{fig:yoremb}
\end{figure}

\paragraph{English phones} As for the TIMIT dimensional space \ref{fig:timitemb}, these patterns are less clear. It is not easy to assign one linguistic feature to the axes, like sonority or backness. The natural groupings of sounds are still observable, but exact composition and number of clusters differs from Yoruba: nasal group separately from approximants and \ipa{/l/}; \ipa{/p/} is separated from other voiceless stops. As for vowels, while \ipa{/ə, ʌ/} and \ipa{/i, ɪ/}  (respectively positional allophones and a tense-lax pair vowel) are grouped together, it can be pointed out that \ipa{/a, o, e/} appear as a detached cluster. Oddly, all of these labels belong only to nuclei of diphthongs, split after data processing, which has to be linked as to why they occupy their own space in the plot. It is thus probable that the differing annotations schemes for both datasets (with more fine-grained distinctions for TIMIT) partly contributes to this.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{timitultphone.png}
    \caption{Timit Phone Embeddings}
    \label{fig:timitemb}
\end{figure}

\paragraph{Yoruba and English} The fact that the clusters differ is somewhat trivial, considering that the UMAP reduces space differently for different subsets of labels, and Yoruba clustering did not have to account for relations with all TIMIT sounds. Hence or whence we also look into the simultaneous projection of Yoruba and English embeddings in one space, by first fitting the transform based on the English data, and then reducing the Yoruba data, collecting Euclidian distances between the common phonemes. After running the projection several times, we find that the distances we obtain are consistent across trials, but it is difficult to extract a common pattern (Figures \ref{fig:embtogether}, \ref{fig:vowel_dist}).

\paragraph{Distances} To investigate this further, we decided to run a correlation test between the Euclidian distance between phones after dimensionality reduction and feature weight. Figures \ref{fig:correng}, \ref{fig:corryor}. This shows that there is a significant correlation between the two variables, with the correlation coefficient being lower for Yoruba than it is for English. Figure \ref{fig:pearson}. This reflects the fact that the English label inventory represents more varied sounds than the Yoruba predicted values, helping establish the correlation pattern during fitting. While this possibly shows that our presentations are reliable and in line with the framework we establish, this remains obscured by the fact that the plot for English is not easily interpretable.CORRECT

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{timitcor.png}
    \caption{Correlation between Euclidian distance of embeddings and Weighted Hamming distance (English)}
    \label{fig:correng}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{yorlastcor.png}
    \caption{Correlation between Euclidian distance of embeddings and Weighted Hamming distance (Yoruba)}
    \label{fig:corryor}
\end{figure}

\section{Discussion}

\subsection{Closer look on confusion and correspondence patterns}
The conditional entropy metric it does not require the predictions to be phonologically accurate, just that they are predicted unambiguously. The drawback of this is that low entropy could arise from systematic bias toward over-represented classes rather than accurate transfer. Let us consider in detail what phonologically non-trivial associations are found in alignments and how they reflect the expected differences between English and Yoruba discussed in Introduction[ref].

\paragraph{PMI} To measure association we employ a symmetric measure related to the conditional entropies. Pointwise Mutual Information (PMI) computes the co-occurence of two variables with respect to the values they would hold if we assumed them to be independent. A higher PMI value therefore indicates a high likelihood that the association is significant. \citep[p.~14]{jurafsky2023} It is calculated by dividing the joint probability by the product of two marginal probabilities. Appendix[ref] In a cross-lingual transfer-learning setting, since two distributions do not have identical event space, we used the co-ocurrence matrix Results[ref] to find the joint and marginal probabilities. The probability of phonemes varies greatly, therefore we normalize PMI by dividing it with negative log likelihood of the joint probability (the theoretical maximum of information content).\citep[p.~61]{bentz2018} NPMI shows what correspondences are statistically meaningful even when the raw counts are extremely low and can be taken as a measure of correlation. Values between -1 and 0 imply that the co-occurrence of the phonemes from two variables in alignments is more scarce than random. On the other hand, values between 0 and 1 suggest that two values co-occur more often than random, so a successful transfer show each class in predicted maps to a certain Yoruba golden label, while labels would just be associated with what is closest in terms of features in the case of ineffective transfer.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{NormalisedPMI.png}
    \caption{Normalized PMI. Turquoise denotes the encoding efficiency gain from aligning, magenta denotes the gain from not aligning. Grey stands for the alignments that have never occurred (we can assume negative values).}
    \label{fig:normpmi}
\end{figure}

\paragraph{Interpretation}

The first look reveals a strong blue diagonal in a lilac field: these are the correspondences between predicted phones and their logical Yoruba counterparts. No suspicious associations dwell in the off-diagonal areas, even among the least frequent phones: phonological closeness rather than noise accounts for the information-carrying phone pairs.

The rows and columns labelled \O respectively corresponding to alignments with deletion and substitutions. Dissassociation between vowels and deletion is particularly high. This is likely  due to the CV syllable structure in Yoruba (Transfer[ref] (3)). The model generates a lot of consonant clusters, in accordance with English phonotactics, which then undergo deletion in order to align the vowels. The frequency of insertions, on the opposite, seems to be unrelated to the phoneme type. One strange exception is Yoruba \ipa{/ɾ\textasciitilde r\textasciitilde ɹ/}(see Trasfet[ref] (4)). The only \textit{ad-hoc} explanation that comes to mind is that our model misses the Yoruba tap realisations, and they are short, or because it predicts a an English rhoticized vowel instead of it. English  Phonemes similar to Yoruba's double-articulated stops, \ipa{/p b k g/}, are occasionally deleted, but no more than other consonants. This likely disproves the hypothesis derived from our cognition from Transfer[ref] (2), that double-articulated stops will be recognized as sequences. A frequent correspondence with labio-velar stops \ipa{ɡ͡b, k͡p} unexpectedly, but logically turned out to be the labio-velar approximant \ipa{w}. Other labial consonants were also associated.

Generally all consonants without close place/manner counterparts show higher entropy. Thus, \ipa{/θ/} is divided equally between \ipa{/f,s,t/}. The rather sonorant \ipa{/ð,ŋ/} are pretty much random. The glottal stop is split between \ipa{/k, h/} and being deleted; \ipa{/p/} is between \ipa{/k͡p,k,f,b/}. These are all examples of migrating to some sort of place/manner compromise. Contrast this with vowels: all vowels, regardless if are in Yoruba labels, have high entropy (see also ref Inference).

A clean tendency is that the VOT is transferred not one-to-one, but regularly: Yoruba lacks voiced fricatives, but when \ipa{/z,ʒ,v/} appear in predictions, they consistently associate with their voiceless pairs, and sometimes with place pairs. Predicted voiced stops map equally to voiceless and voiced Yoruba stops, while voiceless always map to voiceless.

Liquids and nasals form a ``connected cluster'' of confusions; it is possibly but not necessarily related to their partial allophony in Yoruba, where nasality often spreads (see Yoruba to English [ref] (4)). In general, more sonorant phones in TIMIT have higher entropy here, and vowels are even more interrelated, slightly grouping by backness.

Yoruba nasal \ipa{a} is associated with o-like vowels, since these occur allophonically in Yoruba [see Datasets]. Other nasal vowels associate with their oral bases (\ipa{/ĩ/} with i-like), etc. but with weaker certainty. They are aligned with approximants more often: \ipa{/ũ/} with \ipa{/w/}, and \ipa{/ĩ/} with \ipa{/j/}.


\subsection{How to benchmark our results}
A challenge in evaluating our model and drawing conclusions about phonology lies in the circular nature of alignments. We first use theoretical assumptions about phoneme closeness for our alignment algorithm. Then we treat these alignments like the actual correspondence (glossing over the conversion step) to speculate which sounds the model was not able to recognize correctly. These alignments are not in fact related the traceable causal/computational link between in- and output during inference. In a way, the alignment being theoretically sound (ref PMI) is not surprising, because it is derived via theoretical edit distance.

Refer to Appendix[ref] for examples of alignments. Impressionistically, although the model output is clearly linked to the input, the alignments are not always intuitively correct. (show) Indeed, they are proven minimal given the feature model, but it does not guarantee to correspond to a linguistic intuition. This is a common issue in the pure feature-based \textit{Lev} alignments (Dellert, p.c.). More traceable options include alignment with some kind of supervised training on a human-aligned data. \citet{nerbonne2010} reports good alignment results with log-transformed feature weights. Another option is to embed some pre-knowledge of phonotactics in the cost functions by assigning weights to different features: say, to improve resolution of consonant clusters, knowing the Yoruba \textbf{CVN} syllable structure, we can make deleting consonants cheap, while changing consonants to vowels costly.

Combining a feature model with minimal distance leads to the overall fwPER being hard to interpret. For example, we do not know the expected cost for two random phonemes, so, unlike in the regular PER, there is no intuition such as ``every 3rd letter is wrong''. It is also not clear how much minimizing the alignment lowers the fwPER on average.

In order to establish a frame of reference within which we can analyze our fwpER values, we evaluate it on random strings. 
We evaluated 3 different generated ``predictions'' with the same cost settings as in Results(ref).

First, we took the exact lengths of the sentences from our predictions, to simulate some understanding of time, then we tested: (1) generating random sequences of TIMIT labels of that length, sampling from a uniform distribution (``nulligrams'') (2) generating that using the unigram probabilities of the TIMIT labels in our predictions. This way we had the same predictions, but filled with random letters, unrelated to the audio input whatsoever. (3) making a classic bigram model based on actual predictions, generating strings of random length until the EOS symbol.

The PER with the first two random sequences is surprisingly close to ours: \texttt{0.4} and \texttt{0.39}. The difference from the model's PER the distribution and median (Figure \ref{fig:perdist}) is worth noticing: in predictions the positively-skewed bell curve has longer tails on both sides; the random alignments gravitate more towards thee average. (Figures \ref*{fig:nulliprob,fig:uniprob}) The intuition allows us to say that the model predictions are not random: it seems, the random simulations never achieve low PER values like \texttt{0.2}to \texttt{0.3}. A qualitative look at the alignments in appendix [ref] suggests that the prediction outcomes are better and indeed linked to the input sound.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{model_per_distribution.png}
    \caption{PER Distribution}
    \label{fig:perdist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{nulligram_PER_distribution.png}
    \caption{Equal probability PER Distribution}
    \label{fig:nulliprob}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{unigram_PER_distribution.png}
    \caption{Unigram PER Distribution}
    \label{fig:uniprob}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{bigramPERdistribution.png}
    \caption{Bigram PER Distribution}
    \label{fig:biprob}
\end{figure}
																																																																																																																																													

\begin{table}[h]
\centering
\caption{Pearson correlation between vowel distance and another variable}
\label{fig:pearson}
\begin{tabular}{lrr}
\toprule
\textbf{Language} & \textbf{Correlation Coefficient ($r$)} & \textbf{$p$-value} \\
\midrule
English & $0.47$ & $0.0002$ \\
Yoruba  & $0.49$ & $0.004$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{Vowel distances between English and Yoruba (sorted by similarity)}
\label{vowel_distsim}
\centering
\begin{tabular}{>{\ipafont}lr}
\toprule
Vowel & Distance \\
\midrule
u & 0.095 \\
ɡ & 0.170 \\
k & 1.119 \\
d & 1.280 \\
m & 1.424 \\
t & 1.571 \\
j & 1.633 \\
s & 1.800 \\
ɔ & 1.947 \\
ɛ & 1.971 \\
b & 1.993 \\
h & 2.549 \\
n & 2.887 \\
w & 2.907 \\
					
e & 3.059 \\
ʃ & 3.278 \\
o & 3.291 \\
l & 3.363 \\
i & 3.590 \\
a & 3.643 \\
f & 5.945 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{yoruba_eng_plot.png}
    \caption{Vowel distances between English and Yoruba}
    \label{fig:vowel_dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{combined_emb.png}
    \caption{Yoruba and English consonant embeddings plotted in the same space}
    \label{fig:embtogether}
\end{figure}

\section*{Limitations}
For future reference, we believe that our model could be further improved by dedicating more attention to the over-generation problem, and possibly address it at the pre-training stage with a modified pre-training objective (or simply a denoising procedure at the data preprocessing stage), or by investigating whether there are some repeating patterns linked to over-generation. This would then in turn help improve the quality of alignment for the evaluation step. Moreover, having datasets that agree on the granularity on the description would be helpful, as this would facilitate the mapping and reduce the risk of inducing biases.
Although the pre-training stage should in principle be as general as possible, introducing phonotactic constraints through a LM model would likely also contribute to reduce noise during generation.
In regards with the Common Voice dataset specifically, we can also mention the size of the corpus in comparison to the English one, since the 0.25 split we used for English for evaluation is almost twice the size of all the Yoruba splits combined. Moreover, although we deem Epitran to be suitable for the G2P step (its paper mentions it being competitive with the FST-based baselines for ASR), it would be interesting to see whether a more-linguistically informed model, perhaps a WFST would yield a different performance.
Another direction we could was to use the embeddings obtained to see whether they latently contain some information about Yoruba which was left out from the annotations we used. For instance, it would be interesting to see whether tone can be accurately predicted by feeding the final representations into a classifier, investigating whether the architecture was sufficient for the model to internalize this additional information.

We of course encourage the reproduction of this study on different datasets, be it for Yoruba or other languages.

\section*{Conclusion}

We trained an audio to IPA ResNet–BiLSTM CTC recognizer on TIMIT and evaluated it zero-shot on Yoruba with a feature-weighted PER, reaching near-perfect figure of 0.0339 for English validation set. (KIT: add more) Tackling the problem of different inventories, we utilized a modified version of feature-weighted Levenshtein algorithm for evaluation. The system reaches \texttt{0.34} PER on Yoruba (median 0.30), among which over-generation error dominates. Limiting model capacity by decreasing hidden-dimension (thus resulting in a lightweight implementation) greatly improved cross-lingual generalization from English to Yoruba. The depth of our model is less than half of that of the original model, reducing the parameter count from approximately 1.55 million to 0.8 million. This constraint prevented over-fitting to the TIMIT phoneme distribution and forced the encoder to extract lower-entropy, articulattory-level representations. The resulting model preserved contrasts of manner and place of articulation while smoothing over language-specific refinements in vowel space, consistent with the (N)PMI and entropy analyses provided in previous sections.
Another important part of the training was CTC loss function. CTC marginalizes over all monotonic alignments between input frames and label sequences, using a dedicated blank symbol to enable many-to-one and skip alignments. Apart from the over-generation of rare phonemes, which we suspect to reflect extra-linguistic noise, we also believe the processing of pauses to have had an impact. We have deleted pause labels from both English and Yoruba based on the fact that our task does not spot word boundaries, so that assuming pause removal will not undermine the performance. Additional noise reduction step as preprocessing for Yoruba would have yielded better achievements, since noise appearing before and after the utterance was unique to Common Voice Yoruba not TIMIT. 

Analysis of confusion, mapping entropy, and (N)PMI shows that transfer is phonetic rather than random: NPMI table revealed that phonetic diplomacy is captured. The network aligns by feature similarity, negotiating between two phonological systems. The model successfully transferred cues of acoustic similarity, but over-differentiates distinctions that Yoruba neutralizes, where numerous English classes cut through the acoustic spaces of Yoruba. To note, American English has 7 vowel contrasts while the transcription distinguishes 15 acoustically similar allophones. We indeed merged redundant distinction in TIMIT labels, however, coalescing English label classes to a greater extent, making it more aligned with Yoruba labels is expected to enhance the transfer. Correspondences track place/manner similarity, voicelessness transfers more reliably than voicing (due to VOT differences), labio-velar stops often surface as \ipa{/w/}, and English’s finely split vowel space and overly narrow transcription tradition collapses toward Yoruba’s coarser categories. In short, the transfer is smart about sounds, but tone-deaf. Transfer keeps the tendency for consonant clusters, systematically breaking the syllable structure of Yoruba. 
PER results with different n-gram settings indicates a circularity: alignments are theory-driven (based on feature phonology) and not causal chain-driven, which biases the further conclusions about phonology into confirm pre-existing notions(thus, in turn, fixing model's tendency to over-generate in noisy circumstances is the most relevant way to reduce the error). This nature of the alignments remains a problem, as long as the alignments are not checked for the link through the time-aligned test set or for intuitiveness by supervised training or curation. The fwPER also lacks the easy interpretation of PER. Nonetheless, random baselines neither match the low-PER tail nor the skew of model outputs, and bigram generations without length control explode in PER—evidence that predictions are signal-bearing and that length control is critical.

Moving forward, the clearest gains lie in (i) controlling insertions (denoising, decoding constraints, or a light phonotactic LM), (ii) inventory simplification on the source side (merge the gradient vowel/voicing contrasts, redundant in English, keep the discrete consonant contrasts) to sharpen correspondences, and (iii) better alignments (supervised or phonotactic-aware costs) and harmonized annotation granularity. Finally, the learned embeddings invite probing for tone and other Yoruba-specific information absent from the labels.


\bibliographystyle{acl_natbib}
\bibliography{main} % your refs.bib

\appendix
\section{Appendix}
\label{sec:appendix}
\subsection{Appendix. A}

\begin{table}[h!]
\centering
\caption{TIMIT to IPA mapping (folded into two columns)}
\label{timit2ipa}
\begin{tabular}{>{\ttfamily}l >{\ipafont}l >{\ipafont}l @{\hskip 1.5em} >{\ttfamily}l >{\ipafont}l >{\ipafont}l}
\toprule
\textbf{TIMIT} & \textbf{IPA} & \textbf{Our} & \textbf{TIM} & \textbf{IPA} & \textbf{Our} \\
\midrule
aa   & \ipa{ɑ}   & \ipa{ɑ}   & ch   & \ipa{t͡ʃ} & \ipa{t͡ʃ} \\
ae   & \ipa{æ}   & \ipa{æ}   & d    & \ipa{d}   & \ipa{d} \\
ah   & \ipa{ʌ}   & \ipa{ʌ}   & dh   & \ipa{ð}   & \ipa{ð} \\
ao   & \ipa{ɔ}   & \ipa{ɔ}   & dx   & \ipa{ɾ}   & \ipa{r} \\
aw   & \ipa{aw}  & \ipa{a + w} & el   & \ipa{l̩}  & \ipa{l} \\
ay   & \ipa{aj}  & \ipa{a + j} & em   & \ipa{m̩}  & \ipa{m} \\
ax   & \ipa{ə}   & \ipa{ə}   & en   & \ipa{n̩}  & \ipa{n} \\
axr  & \ipa{ə˞}  & \ipa{ə}   & f    & \ipa{f}   & \ipa{f} \\
eh   & \ipa{ɛ}   & \ipa{ɛ}   & g    & \ipa{g}   & \ipa{g} \\
er   & \ipa{ɜ˞}  & \ipa{ɜ˞}  & hh   & \ipa{h}   & \ipa{h} \\
ey   & \ipa{ej}  & \ipa{e + j} & h    & \ipa{h}   & \ipa{h} \\
ih   & \ipa{ɪ}   & \ipa{ɪ}   & jh   & \ipa{d͡ʒ} & \ipa{d͡ʒ} \\
ix   & \ipa{ɨ}   & \ipa{ɨ}   & k    & \ipa{k}   & \ipa{k} \\
iy   & \ipa{i}   & \ipa{i}   & l    & \ipa{l}   & \ipa{l} \\
ow   & \ipa{ow}  & \ipa{o + w} & m    & \ipa{m}   & \ipa{m} \\
oy   & \ipa{ɔj}  & \ipa{ɔ + j} & n    & \ipa{n}   & \ipa{n} \\
uh   & \ipa{ʊ}   & \ipa{ʊ}   & nx   & \ipa{ɾ̃}  & \ipa{n} \\
uw   & \ipa{u}   & \ipa{u}   & ng   & \ipa{ŋ}   & \ipa{ŋ} \\
ux   & \ipa{ʉ}   & \ipa{ʉ}   & p    & \ipa{p}   & \ipa{p} \\
ax-h & \ipa{ə̥}  & \ipa{ə}   & q    & \ipa{ʔ}   & \ipa{ʔ} \\
bcl  & \ipa{b̚}  & \ipa{b}   & r    & \ipa{ɹ}   & \ipa{r} \\
dcl  & \ipa{d̚}  & \ipa{d}   & s    & \ipa{s}   & \ipa{s} \\
eng  & \ipa{ŋ̍}  & \ipa{ŋ}   & sh   & \ipa{ʃ}   & \ipa{ʃ} \\
gcl  & \ipa{ɡ̚}  & \ipa{g}   & t    & \ipa{t}   & \ipa{t} \\
hv   & \ipa{ɦ}   & \ipa{h}   & th   & \ipa{θ}   & \ipa{θ} \\
kcl  & \ipa{k̚}  & \ipa{k}   & v    & \ipa{v}   & \ipa{v} \\
pcl  & \ipa{p̚}  & \ipa{p}   & w    & \ipa{w}   & \ipa{w} \\
tcl  & \ipa{t̚}  & \ipa{t}   & wh   & \ipa{ʍ}   & \ipa{ʍ} \\
pau  & \ipa{|}   & \ipa{–}   & y    & \ipa{j}   & \ipa{j} \\
epi  & \ipa{||}  & \ipa{–}   & z    & \ipa{z}   & \ipa{z} \\
h\#  & \ipa{/}   & \ipa{–}   & zh   & \ipa{ʒ}   & \ipa{ʒ} \\
b    & \ipa{b}   & \ipa{b}   &      &           &         \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Appendix. B}
\label{sec:appendixb}
\begin{table}[h!]
\centering
\caption{Yoruba IPA inventory}
\label{yorubaipa}
\begin{tabular}{>{\itshape}l >{\ipafont}l >{\ipafont}l}
\toprule
\textbf{Yoruba} & \textbf{IPA} & \textbf{IPA (adjusted)}\\
\midrule
m & \ipa{m} & \ipa{m} \\
i & \ipa{i} & \ipa{i} \\
k & \ipa{k} & \ipa{k} \\
y & \ipa{j} & \ipa{j} \\
u & \ipa{u} & \ipa{u} \\
a & \ipa{a} & \ipa{a} \\
w & \ipa{w} & \ipa{w} \\
n & \ipa{n} & \ipa{n} \\
t & \ipa{t} & \ipa{t} \\
l & \ipa{l} & \ipa{l} \\
s & \ipa{s} & \ipa{s} \\
b & \ipa{b} & \ipa{b} \\
e & \ipa{e} & \ipa{e} \\
o & \ipa{o} & \ipa{o} \\
g & \ipa{ɡ} & \ipa{ɡ} \\
h & \ipa{h} & \ipa{h} \\
d & \ipa{d} & \ipa{d} \\
r & \ipa{ɾ} & \ipa{r} \\
f & \ipa{f} & \ipa{f} \\
\d{e} & \ipa{ɛ} & \ipa{ɛ} \\
\d{s} & \ipa{ʃ} & \ipa{ʃ} \\
\d{o} & \ipa{ɔ} & \ipa{ɔ} \\
j & \ipa{d͡ʒ} & \ipa{d͡ʒ} \\
\textasciiacute & \ipa{˦} & \ipa{–} \\
\textasciigrave & \ipa{˨} & \ipa{–} \\
in & \ipa{ĩ} & \ipa{ĩ} \\
un & \ipa{ũ} & \ipa{ũ} \\
gb & \ipa{ɡ͡b} & \ipa{ɡ͡b} \\
p & \ipa{k͡p} & \ipa{k͡p} \\
\d{o}n & \ipa{ɔ̃} & \ipa{ã} \\
\d{e}n & \ipa{ɛ̃} & \ipa{ɛ̃} \\
an & \ipa{ã} & \ipa{ã} \\
-- & \ipa{˧} & \ipa{–} \\
n & \ipa{ŋ} & \ipa{–} \\
\d{u} & \ipa{ʊ} & \ipa{–} \\
\d{i} & \ipa{ɪ} & \ipa{–} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
 \includegraphics[width=1\linewidth]{YorPhonemeFrequency.png}
    \caption{Original Yoruba Dataset Phoneme Frequency}
    \label{fig:yorubaphonefreq}
\end{figure}

\begin{figure}
    \centering
 \includegraphics[width=1\linewidth]{PredPhonemeFrequency.png}
    \caption{After Evaluation Phoneme Frequency}
    \label{fig:predphonefreq}
\end{figure}


\subsection{Appendix. C}

Formula for the substitution cost of two IPA vectors from PanPhon inventory: English $\vec e$ and Yoruba $\vec y$.
\[
\text{dist}(\vec e,\vec y)
=\frac{\bigl|\{\, i \mid (e_i\neq y_i)\wedge (e_i\neq 0 \lor y_i\neq 0)\,\}\bigr|}
       {\bigl|\{\, i \mid e_i\neq 0 \lor y_i\neq 0 \,\}\bigr|}
\]\cite{mortensen2016}

Formulae for confusions.
Formally, let $Y$ denote the gold Yoruba phoneme and $\hat Y$ the predicted TIMIT phoneme var. Each row of the confusion matrix is a probability distribution over predicted labels, conditioned on a given Yoruba label.
\[
P(\hat Y=j\mid Y=i) = \frac{|\{(x: Y(x)=i, \hat Y(x)=j)\}|}{|\{(x: Y(x)=i)\}|},
\]
Then the confusion entropy is $H(\hat Y\mid Y = i)=- \sum_j P(\hat Y=j\mid Y=i)\log_2 P(\hat Y=j\mid Y=i).$

The metric that we call mapping entropy is the opposite. With the posterior probabilities of gold labels given a certain prediction
\[
P(Y=i\mid \hat Y=j)=\frac{|\{(x: Y(x)=i, \hat Y(x)=j)\}|}{|\{(x: \hat Y(x)=j)\}|}
\]
the mapping entropy is $H(Y\mid \hat Y=j)=-\sum_i P(Y=i\mid \hat Y=j)\log_2 P(Y=i\mid \hat Y=j).$ With the expectation $H(Y\mid \hat Y)=\sum_j P(\hat Y=j) H(Y \mid \hat Y = j)$, and perplexity $P(Y\mid \hat Y)=2^{H(Y \mid \hat Y)}$. \cite{Bentz2022}

Formula for normalized PMI for a gold sound $y$ and predicted sound $e$ in the co-occurrence counts matrix obtained from alignments.
$$\text{nPMI}(y, e) = log_2(y, e) - log_2(y) - log_2(e)$$ \cite{jurafsky2023}

\subsection{Appendix D}

\begin{table}[t]
\centering
\caption{Some of the worst predictions (Pred vs.\ Gold).}
\label{tab:worstpred}
\setlength{\tabcolsep}{4pt}
\scriptsize
\begin{tabularx}{\columnwidth}{@{}lYY@{}}
\toprule
& \textbf{Pred} & \textbf{Gold} \\
\midrule
\textbf{W1} &
\ipa{ʔ ɪ t ð ə ʔ ɪ ə b i p ɨ ɪ n d u ə b ɨ ɹ ɛ n ə d ɨ ɹ ɪ k ɨ ɨ t ɪ b ʉ p l ɪ n ɨ n t l ʊ d ʉ ɨ ɹ ɹ ɪ t ɨ ŋ b ʉ d b l ɨ b ʔ e j ɹ t ʉ z d i d d n ɡ ɨ n p ɨ p} &
\ipa{ɡ͡b o ɡ͡b o a w ã d ɔ k i t a t i k͡p ĩ u l a t i w a ɔ n a a b a j ɔ s i a a r u n} \\
\addlinespace[2pt]
\textbf{W2} &
\ipa{ɔ w a w v ə e ʔ j z ʉ w ɪ m ʉ ɹ u u ʔ j i ɹ i ɪ z z ɪ ɪ ə z ʉ ɪ ɨ ɡ u d l ʌ ʌ ʔ ɪ n ɹ i ɪ ɪ z ʉ ɹ h j o} &
\ipa{i k u n i e r e ɛ ʃ ɛ f u n ɛ̃ i t o b a k͡p a e e j a n} \\
\addlinespace[2pt]
\textbf{W3} &
\ipa{n ɛ ə ʔ ə t æ ʔ ɛ ʔ ə ɹ n p ʊ ɪ t ɪ m p ɜ˞ m ɨ ɹ ɨ z b ɹ a j t o ɨ n t a t o w t ə ɹ ɨ p ɨ n ɛ ɹ o w u t ɛ t ɹ ɨ ɹ n ə ʔ æ t k w h i p ɨ n b ə p ɹ ʉ z k ʉ n z ɨ w ɨ d ɨ ŋ ɨ p n æ t ɪ n k ɨ t ə ə n} &
\ipa{o l u ɔ m ɔ l o f a r a ɡ͡b a ɔ b ɛ f u n a a r ɛ n i b i i k͡p o l o n ɡ o i b o} \\
\bottomrule
\end{tabularx}
\end{table}

%----------------------------- Best predictions (stacked) ---------------------
\begin{table}[t]
\centering
\caption{Two best predictions}
\label{tab:bestpred}
\setlength{\tabcolsep}{4pt}
\scriptsize
\begin{tabularx}{\columnwidth}{@{}lY@{}}
\toprule
\textbf{Distance} & 6.9 \\
\textbf{Pred}     & \ipa{u i l e j d i ɑ d u l a w ə m æ d ɑ l æ l i ɹ a j ɨ z s ɔ i ɹ u b ɑ m} \\
\textbf{Gold}     & \ipa{o r i l ɛ e d e a d u l a w ɔ m ɛ t a l a l o m a a n s ɔ e d e j o r u b a} \\
\addlinespace[2pt]
\textbf{Distance} & 6.0 \\
\textbf{Pred}     & \ipa{o l m i ɑ j ɹ i b i ʌ l w k ʌ b ʌ d͡ʒ u l ɪ b ʊ s i i b ɔ w u} \\
\textbf{Gold}     & \ipa{o m i j a l e n i ɔ l ɔ k͡p a a r ɔ d͡ʒ ɔ i b ã s i i ɡ͡b o h o} \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}
    \centering
 \includegraphics[width=1\linewidth]{alignments.png}
    \caption{Automatic alignment of the median prediction}
    \label{fig:alignments}
\end{figure}

\end{document} 