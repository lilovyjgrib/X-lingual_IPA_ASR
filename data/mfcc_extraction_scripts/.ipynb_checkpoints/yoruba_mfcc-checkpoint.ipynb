{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822c24ca-5361-470e-9285-6218d2d89edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Audio\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1208795d-3eb5-4d01-a9c2-22dab5477841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, torch.audio can handle mp3 data\n",
    "# torchaudio.set_audio_backend(\"sox_io\")  # oder \"ffmpeg\" falls installiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfd87c6-98eb-472e-96a9-b69236072a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"yo\")\n",
    "\n",
    "# or load the separate splits if the dataset has train/validation/test splits\n",
    "train_dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"yo\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"yo\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"yo\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfb3b10-300f-4e90-877f-851cf39be597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "    num_rows: 1213\n",
      "})\n",
      "Dataset({\n",
      "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "    num_rows: 863\n",
      "})\n",
      "Dataset({\n",
      "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "    num_rows: 999\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# ds = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"yo\")\n",
    "# printing the structures of the train / valid / test sets\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a94e8fe-2b4f-4b1f-93fe-313026e68b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do I find the right sample rate???\n",
    "SAMPLE_RATE = 16000\n",
    "N_MFCC = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0831f85a-0c09-462f-b3df-4e3978712255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_from_dataset_item(example, sample_rate=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    # Hole die Audiodaten aus dem Beispiel\n",
    "    audio = example[\"audio\"]\n",
    "    waveform = torch.tensor(audio[\"array\"], dtype=torch.float64).unsqueeze(0)  # [1, time]\n",
    "    sr = audio[\"sampling_rate\"]\n",
    "\n",
    "    # Resample falls notwendig\n",
    "    if sr != sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # optional: in float32 for further (torchaudio-Transforms akzeptieren float32 besser)\n",
    "    waveform = waveform.to(torch.float32)\n",
    "\n",
    "    # MFCC-Berechnung\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40}\n",
    "    )\n",
    "    mfcc = mfcc_transform(waveform)\n",
    "\n",
    "    # Delta-Features\n",
    "    delta = torchaudio.functional.compute_deltas(mfcc)\n",
    "    delta2 = torchaudio.functional.compute_deltas(delta)\n",
    "\n",
    "    # Kombinieren\n",
    "    combined = torch.cat([mfcc, delta, delta2], dim=0).squeeze(0)  # [39, time]\n",
    "\n",
    "    return {\"mfcc\": combined.T}  # [time_steps, 39]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557a178a-9619-4918-8518-36c812b1bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
    "dataset = dataset.map(extract_mfcc_from_dataset_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3a4845-0dc6-4026-8e4b-3cb6351653bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# convertingn to a torch compatible list\n",
    "class MFCCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"mfcc\": item[\"mfcc\"],\n",
    "            \"text\": item[\"sentence\"]\n",
    "        }\n",
    "\n",
    "# dataLoader\n",
    "torch_dataset = MFCCDataset(dataset[\"train\"])\n",
    "dataloader = DataLoader(torch_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332f1097-b142-42a3-9f89-4b2fc1490405",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train_data = dataset[\"train\"]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_data_n = \u001b[43mdataset_n\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# sample = train_data[0]\u001b[39;00m\n\u001b[32m      4\u001b[39m sample_n = train_data_n[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_n' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd8dda2-f7b2-41e7-bfd6-77d5ecf29999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(type(dataset[\"train\"]))\n",
    "\n",
    "# train_data = dataset[\"train\"]\n",
    "train_data_n = dataset_n[\"train\"]\n",
    "# sample = train_data[0]\n",
    "sample_n = train_data_n[0]\n",
    "\n",
    "\n",
    "print(sample_n[\"sentence\"])\n",
    "print(torch.tensor(sample_n[\"mfcc\"]).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a3805d-a6ea-4562-a473-ef34d61c9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch combatible dataset\n",
    "# torch_dataset = MFCCDataset(dataset)\n",
    "\n",
    "# local path!!! .py-Datei\n",
    "output_path = Path(\"/home/aaron/automated_speech_recognition/X-lingual_IPA_ASR/data/mfcc_data_export.py\")\n",
    "\n",
    "# extract all data (warning: can be RAM intensiv)\n",
    "# to remember what torch_dataset looks like: torch_dataset = MFCCDataset(dataset[\"train\"])\n",
    "\n",
    "export_data = []\n",
    "for item in torch_dataset:\n",
    "    export_data.append({\n",
    "        \"mfcc\": item[\"mfcc\"], \n",
    "        \"text\": item[\"text\"]\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92162d94-124d-4f37-8022-c321d7198773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved as .py data: /home/aaron/automated_speech_recognition/X-lingual_IPA_ASR/data/mfcc_data_export.py\n"
     ]
    }
   ],
   "source": [
    "# makes dictionary to a valid python text string\n",
    "py_code = \"data = \" + json.dumps(export_data, indent=2)\n",
    "\n",
    "# write data\n",
    "output_path.write_text(py_code, encoding=\"utf-8\")\n",
    "print(f\"data saved as python file: {output_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650b3f93-2063-4efd-8492-44a1765ef677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c6cb0326824c61bd5521670aca2b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4322b55e4644e089c303379ba1cfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe8eb62c5ea42529061c0d0696ba7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cd046831b447aca799b7ce7b96eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e778af003412caaf731c26c8e11c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bd53b1b3554485b58f50306db2c97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# has to be done before the export!!\n",
    "# extraction only mfcc´s + sentence (deleting other informations like client_id, age, gender...)\n",
    "dataset_n = dataset.remove_columns([\n",
    "    col for col in dataset.column_names[\"train\"] \n",
    "    if col not in [\"mfcc\", \"sentence\"]\n",
    "])\n",
    "\n",
    "dataset_n.save_to_disk(\"yoruba_mfcc_dataset\")  # save as arrow format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fafe8c4-8253-4d4f-bcf1-fa1afaf7de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "sentence: Ọmọ ẹgbẹ́ òkùnkùn dèrò àtìmọ́lé torí nílùú Ìbàdàn.\n",
      "MFCC shape: torch.Size([623, 13, 3])\n",
      "MFCC (first row): tensor([[-4.8120e+02,  6.1035e-06, -9.0949e-14],\n",
      "        [ 2.7145e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-3.5849e-05, -7.2760e-13,  1.0842e-20],\n",
      "        [ 3.4594e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.9379e-05,  3.6380e-13, -5.4210e-21],\n",
      "        [ 2.9969e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.2608e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.9617e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.4746e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.7256e-05,  3.6380e-13, -5.4210e-21],\n",
      "        [ 1.5318e-04,  0.0000e+00,  0.0000e+00],\n",
      "        [-8.7926e-05,  1.4552e-12, -2.1684e-20],\n",
      "        [-2.7204e-05,  3.6380e-13, -5.4210e-21]])\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "sentence: Ìyàwó àwọn ọlọ́pàá tó kú lásìkò ìwọ́de tó kọjá ti bẹ̀bẹ̀ fún ìrànwọ́\n",
      "MFCC shape: torch.Size([620, 13, 3])\n",
      "MFCC (first row): tensor([[-4.8199e+02,  6.1035e-06, -9.0949e-14],\n",
      "        [ 1.4012e-05,  1.8190e-13, -2.7105e-21],\n",
      "        [-3.2122e-05, -7.2760e-13,  1.0842e-20],\n",
      "        [ 2.9489e-05,  3.6380e-13, -5.4210e-21],\n",
      "        [-1.3723e-05,  1.8190e-13, -2.7105e-21],\n",
      "        [ 1.3106e-05,  1.8190e-13, -2.7105e-21],\n",
      "        [-2.3146e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.6542e-06, -2.2737e-14,  3.3881e-22],\n",
      "        [-1.8103e-06, -2.2737e-14,  3.3881e-22],\n",
      "        [-2.6149e-05, -3.6380e-13,  5.4210e-21],\n",
      "        [ 1.4812e-04,  0.0000e+00,  0.0000e+00],\n",
      "        [-8.3667e-05, -1.4552e-12,  2.1684e-20],\n",
      "        [-2.6633e-05, -3.6380e-13,  5.4210e-21]])\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "sentence: Nítori ọrọ táa sọ lọ́jọ́sí ni Túndé yoo fi kúrò nílé ìwé.\n",
      "MFCC shape: torch.Size([598, 13, 3])\n",
      "MFCC (first row): tensor([[-4.9423e+02, -6.1035e-06,  9.0949e-14],\n",
      "        [ 2.0291e-05,  3.6380e-13, -5.4210e-21],\n",
      "        [-3.8627e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0098e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.0916e-05, -3.6380e-13,  5.4210e-21],\n",
      "        [ 2.9674e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.8911e-05,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.9355e-06,  0.0000e+00,  0.0000e+00],\n",
      "        [-9.6884e-07,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.5554e-05,  3.6380e-13, -5.4210e-21],\n",
      "        [ 1.5781e-04,  2.9104e-12, -4.3368e-20],\n",
      "        [-8.6474e-05, -1.4552e-12,  2.1684e-20],\n",
      "        [-3.1500e-05,  0.0000e+00,  0.0000e+00]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# example first 3 sentences with mfcc extractions\n",
    "for i in range(3):\n",
    "    sample = dataset[\"train\"][i]\n",
    "    mfcc = torch.tensor(sample[\"mfcc\"])\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"sentence:\", sample[\"sentence\"])\n",
    "    print(\"MFCC shape:\", mfcc.shape)\n",
    "    print(\"MFCC (first row):\", mfcc[0])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b09b5a3-5f72-49d4-8ffa-0904e0085ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 1213\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 863\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "    other: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 1113\n",
      "    })\n",
      "    invalidated: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 229\n",
      "    })\n",
      "    validated: Dataset({\n",
      "        features: ['sentence', 'mfcc'],\n",
      "        num_rows: 3077\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# shape of the dataset, splits etc. \n",
    "print(dataset_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3ee11-db3b-4586-8152-b1fcf1e77d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
