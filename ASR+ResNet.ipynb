{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzQFlleGTFJ2",
        "outputId": "b1b97dc0-e453-4e9a-cdd9-dae710d9226c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount= True)\n",
        "\n",
        "import os\n",
        "os.chdir('./drive/MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9_8U10qZ03H"
      },
      "source": [
        "#### import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og37GU8EYXQt",
        "outputId": "d06dbf6f-e199-4393-9f77-fd403ef1030c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting params\n",
            "  Downloading params-0.9.0-py3-none-any.whl.metadata (631 bytes)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from params) (1.17.0)\n",
            "Downloading params-0.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: params\n",
            "Successfully installed params-0.9.0\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install params\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MBgjbdM6-GvG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import editdistance as ed\n",
        "\n",
        "from collections import defaultdict\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import matplotlib.pyplot\n",
        "from IPython import display\n",
        "from jiwer import wer\n",
        "import params\n",
        "import time\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eh1FR-obeeY"
      },
      "source": [
        "#### read data\n",
        "define help function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5DBThAEHbhhf"
      },
      "outputs": [],
      "source": [
        "def read_TIMIT(path):\n",
        "  '''\n",
        "  args:\n",
        "    path: path of TIMIT data(mfcc features, phoneme labels)\n",
        "  return:\n",
        "    feats: list of list for each audio samples\n",
        "    labels: list of list for each audio samples\n",
        "  '''\n",
        "\n",
        "  feats, labels= [], []\n",
        "  length_feats, length_labels= [], []\n",
        "\n",
        "  # read processed TIMIT data\n",
        "  # list of dictionarys with keys being 'mfcc', 'phonemes', 'path'\n",
        "  samples= torch.load(path, weights_only= False)\n",
        "  for idx in range(len(samples)):\n",
        "    feats.append(samples[idx]['mfcc'])\n",
        "    labels.append([phoneme.strip() for phoneme in samples[idx]['phonemes']])\n",
        "  return feats, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgNIqdSed4aw"
      },
      "source": [
        "execute and review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l2g0WQoKorH",
        "outputId": "39f3aeec-7435-4ee7-9c31-ae65c10d4e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCC feature matrix Shape (one audio sample):\t(82, 39)\n",
            "phoneme labels(one audio sample):\t['h#', 'dh', 'ix', 's', 'pcl', 'p', 'iy', 'tcl', 'ch', 's', 'em', 'pcl', 'p', 'ow', 'z', 'iy', 'ax', 'm', 'pau', 'm', 'ay', 'tcl', 'b', 'ax-h', 'gcl', 'g', 'ih', 'n', 'm', 'ah', 'n', 'dcl', 'd', 'ey', 'h#']\n"
          ]
        }
      ],
      "source": [
        "path= r'timit_mfcc_data.pt'\n",
        "feats, labels= read_TIMIT(path)\n",
        "\n",
        "\n",
        "# check mfcc feature matrix dimension\n",
        "print(f'MFCC feature matrix Shape (one audio sample):\\t{feats[-1].shape}')\n",
        "# check IPA repository\n",
        "print(f'phoneme labels(one audio sample):\\t{labels[-1]}')\n",
        "\n",
        "# seems like 'h#' marks sos and eos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXGaIdZoL6VY",
        "outputId": "120076a2-6cd9-4b42-82e1-1f7eece68bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_frames:\t78\n",
            "num_labels:\t35\n"
          ]
        }
      ],
      "source": [
        "print('\\n'.join([f'num_frames:\\t{len(feats[0])}', f'num_labels:\\t{len(labels[0])}']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AthoPcjfR2Xk"
      },
      "outputs": [],
      "source": [
        "# mark max length for mfcc features and labels\n",
        "max_len_feats= max([len(feat) for feat in feats])\n",
        "max_len_labels= max([len(label) for label in labels])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVCgFXtn_KKa"
      },
      "source": [
        "Process Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMytMx8u_LwX"
      },
      "source": [
        "handle h#, pau, epi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RMIJegZA_OEB"
      },
      "outputs": [],
      "source": [
        "# simply remove them\n",
        "labels = [[symbol for symbol in label if symbol not in 'h# epi pau'] for label in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2F4Ce37M8CU"
      },
      "source": [
        "handle DIPHTHONGS, SIMILAR SOUNDS in labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wgcxU-NhFg9O"
      },
      "outputs": [],
      "source": [
        "# diphthongs\n",
        "\n",
        "diphthongs= ['ey', 'aw', 'ay', 'ow']\n",
        "# ey 'bait' -> split\n",
        "# aw 'bout' -> split\n",
        "# ay 'bite' -> split\n",
        "# oy 'boy' -> oh + y\n",
        "# ow 'boat' -> split\n",
        "\n",
        "diphthong_regex= re.compile('|'.join(sorted(map(re.escape, diphthongs),\n",
        "                                              key= len, reverse= True)))\n",
        "oy_regex= re.compile('oy')\n",
        "\n",
        "def split_diphthongs(label):\n",
        "  label= ' '.join(label)\n",
        "  label= diphthong_regex.sub(lambda x: ' '.join(x.group()), label)\n",
        "  label = oy_regex.sub('oh y', label).split()\n",
        "  return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jdyY-LMV_CD4"
      },
      "outputs": [],
      "source": [
        "# similar sounds\n",
        "\n",
        "\n",
        "merge_ipa= {\n",
        "    # marginal sounds\n",
        "    'ax-h': 'ax',\n",
        "    'bcl': 'b',\n",
        "    'dcl': 'd',\n",
        "    'gcl': 'g',\n",
        "    'kcl': 'k',\n",
        "    'pcl': 'p',\n",
        "    'tcl': 't',\n",
        "\n",
        "    'en': 'n',\n",
        "    'em': 'm',\n",
        "    'el': 'l',\n",
        "    'eng': 'ng',\n",
        "\n",
        "    ## /ɹ/ sound\n",
        "    # 'axr': 'r' ? 'ɹ' ?\n",
        "    # 'dx': 'r',\n",
        "    # 'nx': 'r',\n",
        "    # 'er': 'r', 'ɹ' ?\n",
        "\n",
        "    # /h/ sound\n",
        "    'hh': 'h',\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW0xzKRxM5kl"
      },
      "source": [
        "execute and review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yLRWnsd7cUQ5"
      },
      "outputs": [],
      "source": [
        "# split diphthongs\n",
        "labels = [split_diphthongs(label) for label in labels]\n",
        "labels_merge = [[merge_ipa.get(symbol, symbol) for symbol in label] for label in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Kmk85sS5AXRP"
      },
      "outputs": [],
      "source": [
        "# handling bcl, dcl, gcl, kcl, pcl, tcl\n",
        "  # used as abbreviation for voiceless stop or plosive with closure\n",
        "  # e.g. bcl : bilbial closure, tcl : alveolar closure\n",
        "\n",
        "# after merging, there may appear identical consecutive phones\n",
        "\n",
        "# if dcl and d > d\n",
        "# if dcl was by itself > d\n",
        "\n",
        "labels_final = []\n",
        "\n",
        "for label, label_merge in zip(labels, labels_merge):\n",
        "  label = ' '.join(label)\n",
        "  label_merge = ' '.join(label_merge)\n",
        "  for key, item in merge_ipa.items():\n",
        "    if ' '.join([key, item]) in label:\n",
        "      label_merge = re.sub(' '.join([item, item]), item, label_merge)\n",
        "\n",
        "  labels_final.append(label_merge.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5lGHnAm-EKN",
        "outputId": "55f8e5f4-130c-4428-9a0b-eb81cf7a1b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sh uw w ax z hh o w l dx ix ng q aa nx uh hv ih z r aa kcl k w ax dh w ah n hv ae n dcl d\n",
            "sh uw w ax z h o w l dx ix ng q aa nx uh hv ih z r aa k k w ax dh w ah n hv ae n d d\n",
            "sh uw w ax z h o w l dx ix ng q aa nx uh hv ih z r aa k w ax dh w ah n hv ae n d\n",
            "b er th dcl d e y pcl aa r dx iy z hh eh v kcl k ah pcl k e y kcl k s ix nx a y s kcl k r iy m\n",
            "b er th d d e y p aa r dx iy z h eh v k k ah p k e y k k s ix nx a y s k k r iy m\n",
            "b er th d e y p aa r dx iy z h eh v k ah p k e y k s ix nx a y s k r iy m\n",
            "s ah m w ax m ix ng gcl g eh dx ix r ih l th r ih l a w dx ax v hv a w s w axr kcl k\n",
            "s ah m w ax m ix ng g g eh dx ix r ih l th r ih l a w dx ax v hv a w s w axr k k\n",
            "s ah m w ax m ix ng g eh dx ix r ih l th r ih l a w dx ax v hv a w s w axr k\n",
            "dh ix pcl e y sh ix n tcl t q eh n dh ax s er dcl jh ix nx axr bcl b o w th axr kcl k uw pcl r eh dx ix ng f axr m dh ix l eh ng th iy ah pcl er e y sh en\n",
            "dh ix p e y sh ix n t t q eh n dh ax s er d jh ix nx axr b b o w th axr k k uw p r eh dx ix ng f axr m dh ix l eh ng th iy ah p er e y sh n\n",
            "dh ix p e y sh ix n t q eh n dh ax s er d jh ix nx axr b o w th axr k uw p r eh dx ix ng f axr m dh ix l eh ng th iy ah p er e y sh n\n",
            "d r ao q iy tcl ch gcl g r ae f ah nx e y n uw ae kcl k s ix s\n",
            "d r ao q iy t ch g g r ae f ah nx e y n uw ae k k s ix s\n",
            "d r ao q iy t ch g r ae f ah nx e y n uw ae k s ix s\n",
            "en ah l ao ng r ah n ix tcl e y z tcl t ix bcl b a y kcl k w ao l ax dx ix kcl k l o w dh ix ng\n",
            "n ah l ao ng r ah n ix t e y z t t ix b b a y k k w ao l ax dx ix k k l o w dh ix ng\n",
            "n ah l ao ng r ah n ix t e y z t ix b a y k w ao l ax dx ix k l o w dh ix ng\n",
            "sh ix q o w ix zh dcl jh o w kcl k s bcl b ah tcl t ux m ah tcl ch gcl g aa r l ix kcl k ix nx ix z f uw dcl d\n",
            "sh ix q o w ix zh d jh o w k k s b b ah t t ux m ah t ch g g aa r l ix k k ix nx ix z f uw d d\n",
            "sh ix q o w ix zh d jh o w k s b ah t ux m ah t ch g aa r l ix k ix nx ix z f uw d\n",
            "d ah nx ae s m ih kcl k eh r iy ix nx ix q oh y l iy r ae gcl g l a y kcl dh ae tcl\n",
            "d ah nx ae s m ih k k eh r iy ix nx ix q oh y l iy r ae g g l a y k dh ae t\n",
            "d ah nx ae s m ih k eh r iy ix nx ix q oh y l iy r ae g l a y k dh ae t\n",
            "q ix dx ih z iy s e y m o w l s e y m t eh l m iy ix tcl t s n e y m\n",
            "q ix dx ih z iy s e y m o w l s e y m t eh l m iy ix t t s n e y m\n",
            "q ix dx ih z iy s e y m o w l s e y m t eh l m iy ix t s n e y m\n",
            "sh ix hv ae dcl jh er dcl d aa r kcl k s ux tcl q ix ng gcl g r iy s ix w ao sh w ao dx axr q ao l y ix axr\n",
            "sh ix hv ae d jh er d d aa r k k s ux t q ix ng g g r iy s ix w ao sh w ao dx axr q ao l y ix axr\n",
            "sh ix hv ae d jh er d aa r k s ux t q ix ng g r iy s ix w ao sh w ao dx axr q ao l y ix axr\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(*labels[i])\n",
        "  print(*labels_merge[i])\n",
        "  print(*labels_final[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_WPafH-m_wwf"
      },
      "outputs": [],
      "source": [
        "# redefine labels and the dictionary\n",
        "labels= labels_final\n",
        "\n",
        "# split train / dev dataset\n",
        "train_feats, dev_feats, train_labels, dev_labels= train_test_split(feats, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRPHzmYKjn-"
      },
      "source": [
        "#### create IPA dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqvguFlNKd8l"
      },
      "source": [
        "define help function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rSTzPtvEKdZe"
      },
      "outputs": [],
      "source": [
        "def create_IPAdictionary(labels):\n",
        "  '''\n",
        "  args:\n",
        "    labels: list of list containing sequence of label for each audio sample\n",
        "  return: ipa2idx\n",
        "        dictionary of IPA_label to index\n",
        "  '''\n",
        "  ipas= set()\n",
        "  for label in labels:\n",
        "    ipas= ipas.union(set(label))\n",
        "  ipas= sorted(ipas)\n",
        "\n",
        "  ipa2idx= {ipa:(idx+2) for idx, ipa in enumerate(ipas)}\n",
        "  ipa2idx['<blank>']= 0\n",
        "\n",
        "  return ipa2idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR9kwdQSLldm"
      },
      "source": [
        "execute and review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GkcimhKjMr",
        "outputId": "3adb7a5d-b8f4-43c6-9f7c-36c44cd28f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<blank> a aa ae ah ao ax axr b ch d dh dx e eh er f g h hv ih ix iy jh k l m n ng nx o oh p q r s sh t th uh uw ux v w y z zh\n"
          ]
        }
      ],
      "source": [
        "ipa2idx= create_IPAdictionary(labels)\n",
        "print(*sorted(ipa2idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ValF7gQOpW5",
        "outputId": "d21aa1b1-17bf-4f29-81c2-c13d0f65f516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of IPA labels in TIMIT:\t 47\n",
            "Index of blank symbol <blank>\":\t 0\n"
          ]
        }
      ],
      "source": [
        "print(f'the number of IPA labels in TIMIT:\\t {len(ipa2idx)}')\n",
        "print(f'Index of blank symbol <blank>\":\\t {ipa2idx[\"<blank>\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mX_mm_nWC0Rw"
      },
      "outputs": [],
      "source": [
        "# compare phonemic / phonetic symbols between\n",
        "# provided TIMIT phonecode and transcription data\n",
        "# https://catalog.ldc.upenn.edu/docs/LDC96S32/PHONCODE.TXT\n",
        "\n",
        "existing= set(ipa2idx.keys())\n",
        "\n",
        "phonecode= set(['b', 'd', 'g', 'p', 't', 'k', 'dx', 'q', # stops\n",
        "             'jh', 'ch', # affricates\n",
        "             's', 'sh', 'z', 'zh', 'f', 'th', 'v', 'dh', # frcatives\n",
        "             'm', 'n', 'ng', 'em', 'en', 'eng', 'nx', # nasals\n",
        "             'l', 'r', 'w', 'y', 'hh', 'hv', 'el', # semivowels and glides\n",
        "             'iy', 'ih', 'eh', 'ey', 'ae', 'aa', 'aw', 'ay', 'ah', 'ao', 'oy', 'ow', 'uh', 'uw', 'ux', 'er', 'ax', 'ix', 'axr', 'ax-h', # vowels\n",
        "             'pau', 'epi', 'h#', '1', '2',# others # epi: epenthetic silence # h# : begin/ end marker # 1 : primary stress marker # 2 : secondary stress marker\n",
        "             ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDnTZeQJDNIG",
        "outputId": "a7357b8a-4b40-44f6-ac2d-63084a9e72f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<blank> a e h o oh\n",
            "1 2 aw ax-h ay el em en eng epi ey h# hh ow oy pau\n"
          ]
        }
      ],
      "source": [
        "print(*sorted((existing - phonecode))) # symbols that does not exist in phonecode but in TIMIT corpus\n",
        "print(*sorted((phonecode - existing))) # symbols that does not exist (or is removed from) in TIMIT corpus but in phonecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ecb_pW9FDQ1S"
      },
      "outputs": [],
      "source": [
        "# review\n",
        "# in TIMIT\n",
        "  # there is no stress markers(not important to our project)\n",
        "  # we intentionally removed epi, h# and pause\n",
        "  # we intentionally split the diphthongs (aw, ay, ey, ow, oy)\n",
        "  # we intentionally merged (1) ax-h, hh to h (2) el to l (3) en eng to n (4) pau to <blank>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvm1LJp7Pz5D"
      },
      "source": [
        "#### Dataset + pad\n",
        "define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v4C5kAvFP6ED"
      },
      "outputs": [],
      "source": [
        "class PhonemeASRDataset(Dataset):\n",
        "  def __init__(self, feats, labels, ipa2idx):\n",
        "    super(PhonemeASRDataset, self).__init__()\n",
        "    self.feats, self.labels= feats, labels\n",
        "    self.ipa2idx= ipa2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.feats)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      feat, label= self.feats[idx], self.labels[idx]\n",
        "      label= [ipa2idx[ipa] for ipa in label]\n",
        "\n",
        "      return torch.tensor(feat), torch.tensor(label, dtype= torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7NqEpoNTGW_"
      },
      "source": [
        "define padding function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S2BlBMgKS99x"
      },
      "outputs": [],
      "source": [
        "def pad_collate(batch, pad_value_feat= 0, pad_value_label= 0):\n",
        "    '''\n",
        "      for collate_fn in DataLoader function\n",
        "\n",
        "    args:\n",
        "      batch: a list of tuples (mfcc, label)\n",
        "      return: padded_mfccs, padded_labels, input_lengths, target_lengths\n",
        "    '''\n",
        "\n",
        "    mfccs, labels= zip(*batch)\n",
        "\n",
        "    # find max length for mfcc(time step) and label in the current batch\n",
        "    max_len_feats= max(mfcc.shape[0] for mfcc in mfccs)\n",
        "    max_len_labels= max(label.shape[0] for label in labels)\n",
        "\n",
        "    # pad mfcc matrices and labels\n",
        "    padded_mfccs= [F.pad(mfcc, (0, 0, 0, max_len_feats - mfcc.shape[0]), value= pad_value_feat) for mfcc in mfccs]\n",
        "    padded_labels= [F.pad(label, (0, max_len_labels - label.shape[0]), value= pad_value_label) for label in labels]\n",
        "\n",
        "    # calculate lengths of input and target lengths\n",
        "    input_lengths = torch.tensor([mfcc.shape[0] for mfcc in padded_mfccs], dtype = torch.long)\n",
        "    target_lengths = torch.tensor([label.shape[0] for label in padded_labels], dtype = torch.long)\n",
        "\n",
        "\n",
        "    # Stack the padded tensors\n",
        "    padded_mfccs= torch.stack(padded_mfccs)\n",
        "    padded_labels= torch.stack(padded_labels)\n",
        "\n",
        "    return padded_mfccs, padded_labels, input_lengths, target_lengths\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9QDDW8XQ3xw"
      },
      "source": [
        "execute and review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LMKQ1fdJJGq2"
      },
      "outputs": [],
      "source": [
        "train_ds= PhonemeASRDataset(train_feats, train_labels, ipa2idx= ipa2idx)\n",
        "train_loader= DataLoader(train_ds, batch_size= 1, # can adjust\n",
        "                          shuffle= True, collate_fn= pad_collate) # yields batch_size x max_len x num_feats as one training batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TsTyyh5W3Sj",
        "outputId": "c29c7c13-b7dc-4480-ca83-1e945c69ae01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 102, 39]) torch.Size([1, 40])\n"
          ]
        }
      ],
      "source": [
        "a, b, _, _ = next(iter(train_loader))\n",
        "print(a.shape, b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liKPX49eTnS3"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PGb4hvDTsKQ"
      },
      "source": [
        "define utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mW795ILQTtcj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def evaluate_PER(model, data_loader):\n",
        "    model.eval()\n",
        "    total_phonemes = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, input_lengths, target_lengths in data_loader:\n",
        "            x, y = x.to(SETTING['device']), y.to(SETTING['device'])\n",
        "            input_lengths = compute_cnn_output_lengths(model, input_lengths).to(SETTING['device'])\n",
        "            target_lengths = target_lengths.to(SETTING['device'])\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs = model(x)  # (B, T, C)\n",
        "\n",
        "            # Get best path (greedy decoding) by argmax over classes\n",
        "            preds = log_probs.argmax(dim=-1)  # (B, T)\n",
        "\n",
        "            # Collapse repeated and remove blanks (assuming blank=0)\n",
        "            # This is a common CTC decoding post-processing:\n",
        "            def decode_ctc(pred_seq):\n",
        "                prev = None\n",
        "                decoded = []\n",
        "                for p in pred_seq:\n",
        "                    if p != prev and p != 0:  # skip blanks and repeated\n",
        "                        decoded.append(p.item())\n",
        "                    prev = p\n",
        "                return decoded\n",
        "\n",
        "            for i in range(preds.size(0)):\n",
        "                pred_seq = preds[i, :input_lengths[i]]  # cut to valid length\n",
        "\n",
        "                pred_decoded = decode_ctc(pred_seq)\n",
        "                target_seq = y[i, :target_lengths[i]].tolist()\n",
        "\n",
        "                # Calculate phoneme error (Levenshtein distance)\n",
        "                errors = levenshtein_distance(pred_decoded, target_seq)\n",
        "                total_errors += errors\n",
        "                total_phonemes += len(target_seq)\n",
        "\n",
        "    PER = total_errors / total_phonemes if total_phonemes > 0 else 0.0 ## remove total_phonemes if total_phonemes > 0\n",
        "    return PER\n",
        "\n",
        "\n",
        "def levenshtein_distance(seq1, seq2):\n",
        "    # classic DP edit distance\n",
        "    m, n = len(seq1), len(seq2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if seq1[i - 1] == seq2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(\n",
        "                    dp[i - 1][j],    # deletion\n",
        "                    dp[i][j - 1],    # insertion\n",
        "                    dp[i - 1][j - 1] # substitution\n",
        "                )\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "\n",
        "def compute_cnn_output_lengths(model, input_lengths):\n",
        "    \"\"\"\n",
        "    Computes the output time lengths after passing through the CNN part\n",
        "    of ASRModel (init_conv + res_blocks).\n",
        "    \"\"\"\n",
        "    if isinstance(input_lengths, torch.Tensor):\n",
        "        input_lengths = input_lengths.tolist()\n",
        "\n",
        "    new_lengths = []\n",
        "    for length in input_lengths:\n",
        "        dummy = torch.zeros(1, model.init_conv[0].in_channels, length)\n",
        "        with torch.no_grad():\n",
        "            out = model.init_conv(dummy)\n",
        "            out = model.res_blocks(out)\n",
        "        new_lengths.append(out.shape[-1])\n",
        "    return torch.tensor(new_lengths, dtype=torch.long)\n",
        "\n",
        "\n",
        "def beam_search_decoder(probs, beam_width=10, blank=0):\n",
        "    \"\"\"\n",
        "    Beam search decoder for CTC.\n",
        "    probs: (time, num_classes) - log probabilities\n",
        "    \"\"\"\n",
        "    T, V = probs.shape\n",
        "    beams = [(tuple(), 0.0)]  # (prefix, score)\n",
        "\n",
        "    for t in range(T):\n",
        "        new_beams = defaultdict(lambda: -float(\"inf\"))\n",
        "        for prefix, score in beams:\n",
        "            for c in range(V):\n",
        "                p = probs[t, c].item()\n",
        "                new_prefix = prefix + (c,)\n",
        "                new_beams[new_prefix] = max(new_beams[new_prefix], score + p)\n",
        "        # Keep top beam_width\n",
        "        beams = sorted(new_beams.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    # Collapse repeats & remove blanks\n",
        "    best_seq, _ = beams[0]\n",
        "    collapsed = []\n",
        "    prev = None\n",
        "    for c in best_seq:\n",
        "        if c != blank and c != prev:\n",
        "            collapsed.append(c)\n",
        "        prev = c\n",
        "    return collapsed\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=  10, delta= 1e-5, mode= 'max'):\n",
        "    self.patience=  patience # the number of epochs to wait observe loss\n",
        "    self.counter= 0\n",
        "    self.best_score= None\n",
        "    self.early_stop= False\n",
        "    self.delta= delta\n",
        "    self.mode= mode\n",
        "\n",
        "  def __call__(self, current):\n",
        "    if self.best_score is None:\n",
        "      self.best_score= current\n",
        "      return False\n",
        "\n",
        "    improvement= (current - self.best_score) if self.mode ==  \"max\" else (self.best_score - current)\n",
        "\n",
        "    if improvement <=  self.delta:\n",
        "      self.counter +=  1\n",
        "      if self.counter >=  self.patience:\n",
        "        self.early_stop= True\n",
        "    else:\n",
        "      self.best_score= current\n",
        "      self.counter= 0\n",
        "\n",
        "    return self.early_stop\n",
        "\n",
        "def get_loader(feat_path, label_path):\n",
        "  dataset = Dataset(\n",
        "      feat_path,\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Zq2r-ZTohh"
      },
      "source": [
        "define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Tehl-PMVGsPb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PyTorch conversion of the TensorFlow/Keras ResNet-BiRNN ASR model\n",
        "Original Author (TF version): Manish Dhakal\n",
        "Converted & extended with CTC + beam search decoding: [Your Name]\n",
        "Year: 2025\n",
        "\"\"\"\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_cnn_layers, cnn_filters, cnn_kernel_size, use_resnet=True):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.use_resnet = use_resnet\n",
        "        layers = []\n",
        "        for _ in range(num_cnn_layers):\n",
        "            layers.append(nn.Conv1d(cnn_filters, cnn_filters, cnn_kernel_size, padding=cnn_kernel_size // 2))\n",
        "            layers.append(nn.BatchNorm1d(cnn_filters))\n",
        "            layers.append(nn.PReLU())\n",
        "        self.res_block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.res_block(x)\n",
        "        if self.use_resnet:\n",
        "            return x + res\n",
        "        return res\n",
        "\n",
        "\n",
        "class ASRModel(nn.Module):\n",
        "    def __init__(self, ip_channel, num_classes, num_res_blocks=3, num_cnn_layers=1, cnn_filters=50,\n",
        "                 cnn_kernel_size=15, num_rnn_layers=2, rnn_dim=170, num_dense_layers=1,\n",
        "                 dense_dim=300, use_birnn=True, use_resnet=True, rnn_type=\"lstm\", rnn_dropout=0.15):\n",
        "        super(ASRModel, self).__init__()\n",
        "\n",
        "        # Initial Conv layer\n",
        "        self.init_conv = nn.Sequential(\n",
        "            nn.Conv1d(ip_channel, cnn_filters, cnn_kernel_size, padding=cnn_kernel_size // 2),\n",
        "            nn.BatchNorm1d(cnn_filters),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            *[ResBlock(num_cnn_layers, cnn_filters, cnn_kernel_size, use_resnet) for _ in range(num_res_blocks)]\n",
        "        )\n",
        "\n",
        "        # RNN layers\n",
        "        rnn_module = nn.GRU if rnn_type.lower() == \"gru\" else nn.LSTM\n",
        "        rnn_input_dim = cnn_filters\n",
        "        self.rnns = nn.ModuleList()\n",
        "        for _ in range(num_rnn_layers):\n",
        "            if use_birnn:\n",
        "                self.rnns.append(rnn_module(rnn_input_dim, rnn_dim, batch_first=True, dropout=rnn_dropout, bidirectional=True))\n",
        "                rnn_input_dim = rnn_dim * 2\n",
        "            else:\n",
        "                self.rnns.append(rnn_module(rnn_input_dim, rnn_dim, batch_first=True, dropout=rnn_dropout))\n",
        "                rnn_input_dim = rnn_dim\n",
        "\n",
        "        # Dense layers\n",
        "        dense_layers = []\n",
        "        dense_in_dim = rnn_input_dim\n",
        "        for _ in range(num_dense_layers):\n",
        "            dense_layers.append(nn.Linear(dense_in_dim, dense_dim))\n",
        "            dense_layers.append(nn.ReLU())\n",
        "            dense_in_dim = dense_dim\n",
        "        self.dense_layers = nn.Sequential(*dense_layers)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_layer = nn.Linear(dense_in_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2) # (B, T, F) -> (B, F, T)\n",
        "        x = self.init_conv(x)\n",
        "        x = self.res_blocks(x)\n",
        "\n",
        "        x = x.transpose(1, 2) # -> (B, T, F)\n",
        "        for rnn in self.rnns:\n",
        "            x, _ = rnn(x)\n",
        "\n",
        "        x = self.dense_layers(x)\n",
        "        x = self.out_layer(x)\n",
        "        x = F.log_softmax(x, dim=-1)  # log_softmax for CTC Loss\n",
        "        return x  # (B, T, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eus4aJVQUerc"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqdeLqiJUf02"
      },
      "source": [
        "define train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "8wYIMKYSUg7-"
      },
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = []\n",
        "\n",
        "    inner_loop = tqdm(train_loader, desc='Batch', leave=False, position=0)\n",
        "\n",
        "    for x, y, input_lengths, target_lengths in inner_loop:\n",
        "      # Check for NaNs/Infs before placing it to device\n",
        "      if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "        print(\"NaN or Inf in input!\")\n",
        "        raise ValueError(\"Invalud model input\")\n",
        "\n",
        "      # place data to device\n",
        "      x, y = x.to(SETTING[\"device\"]), y.to(SETTING[\"device\"])\n",
        "      input_lengths = compute_cnn_output_lengths(model, input_lengths).to(SETTING[\"device\"])\n",
        "      target_lengths = target_lengths.to(SETTING[\"device\"])\n",
        "\n",
        "      log_probs = model(x)  # (B, T, C)\n",
        "      log_probs = log_probs.transpose(0, 1)  # (T, B, C)\n",
        "\n",
        "      # Check for NaNs/Infs before loss\n",
        "      if torch.isnan(log_probs).any() or torch.isinf(log_probs).any():\n",
        "          print(\"NaN or Inf in model output!\")\n",
        "          raise ValueError(\"Invalid model output\")\n",
        "\n",
        "      y_concat = torch.cat([y[i][:target_lengths[i]] for i in range(y.size(0))])\n",
        "      # calculate loss\n",
        "      loss = loss_fn(log_probs, y_concat, input_lengths, target_lengths)\n",
        "\n",
        "      # Check for NaNs/Infs among loss\n",
        "      if torch.isnan(loss) or torch.isinf(loss) or torch.isnan(log_probs).any() or torch.isinf(log_probs).any():\n",
        "        print(\"NaN or Inf in loss!\")\n",
        "        raise ValueError(\"Invalid loss\")\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      total_loss.append(loss.item())\n",
        "\n",
        "    return sum(total_loss) / len(total_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "iytJhwDnHBGU"
      },
      "outputs": [],
      "source": [
        "#------------------------------- Traing Settings -------------------------------#\n",
        "SETTING= {\n",
        "    \"seed\": 43,\n",
        "    \"learning_rate\": 1e-5, ## gradient exploded at epoch 5/70\n",
        "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"batch_size\": 64,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_epochs\": 70,\n",
        "    \"num_workers\": 2,\n",
        "    \"pin_memory\": True,\n",
        "    \"load_model\": False,\n",
        "    \"load_model_file\": \"./drive/MyDrive/ResLSTM.path.tar\",\n",
        "    \"patience\": 10,\n",
        "#    \"feat_dir\": directory of features\n",
        "#    \"label_dir\": direct1ory of labels\n",
        "\n",
        "}\n",
        "\n",
        "torch.manual_seed(SETTING[\"seed\"])\n",
        "\n",
        "\n",
        "#------------------------------- DataLoader -------------------------------#\n",
        "\n",
        "train_ds= PhonemeASRDataset(train_feats, train_labels, ipa2idx= ipa2idx)\n",
        "dev_ds= PhonemeASRDataset(dev_feats, dev_labels, ipa2idx= ipa2idx)\n",
        "# test_ds =  test 어쩌구\n",
        "\n",
        "train_loader= DataLoader(train_ds,\n",
        "                          batch_size= SETTING[\"batch_size\"],\n",
        "                          shuffle= True, collate_fn= pad_collate,\n",
        "                         num_workers= SETTING[\"num_workers\"],\n",
        "                         pin_memory= SETTING[\"pin_memory\"]) # yields (B, T, C)\n",
        "dev_loader= DataLoader(dev_ds,\n",
        "                      batch_size= SETTING[\"batch_size\"],\n",
        "                       shuffle= False, collate_fn= pad_collate,\n",
        "                       num_workers= SETTING[\"num_workers\"],\n",
        "                      pin_memory= SETTING[\"pin_memory\"])\n",
        "\n",
        "early_stopping =  EarlyStopping(patience= SETTING[\"patience\"], delta= 0.001, mode =  \"max\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLaiL6Ak5ZtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_7qkfNLHGtda"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 39\n",
        "NUM_CLASSES = 47\n",
        "\n",
        "model = ASRModel(\n",
        "    ip_channel=INPUT_DIM,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    num_res_blocks=5,\n",
        "    num_cnn_layers=2,\n",
        "    cnn_filters=50,\n",
        "    cnn_kernel_size=15,\n",
        "    num_rnn_layers=2,\n",
        "    rnn_dim=170,\n",
        "    num_dense_layers=1,\n",
        "    dense_dim=340,\n",
        "    use_birnn=True,\n",
        "    rnn_type=\"lstm\",\n",
        "    rnn_dropout=0.15\n",
        ").to(SETTING[\"device\"])\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = nn.CTCLoss(blank=0, zero_infinity=True, reduction='sum')  # blank token idx=0\n",
        "optimizer = optim.Adam(model.parameters(), lr=SETTING['learning_rate'], weight_decay=SETTING['weight_decay'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5flBbOj_HDKg",
        "outputId": "6c964e4b-d750-4938-effc-727ef0221a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/70 [00:00<?, ?it/s]\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:04<04:09,  4.62s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:09<04:00,  4.54s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:16<05:01,  5.80s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:20<04:24,  5.18s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:25<04:19,  5.20s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:30<03:59,  4.90s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:34<03:39,  4.58s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:38<03:25,  4.38s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:44<03:46,  4.92s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:47<03:26,  4.59s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:51<03:10,  4.33s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [00:57<03:20,  4.65s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [01:01<03:06,  4.44s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:04<02:51,  4.18s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:11<03:17,  4.94s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:15<03:03,  4.71s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:19<02:50,  4.48s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:25<02:59,  4.85s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:29<02:48,  4.67s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:34<02:46,  4.75s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:39<02:48,  4.95s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:44<02:38,  4.82s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:48<02:25,  4.54s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:53<02:25,  4.70s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [01:57<02:13,  4.44s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [02:01<02:04,  4.28s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:05<02:05,  4.47s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:11<02:08,  4.77s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:15<02:00,  4.62s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:21<02:01,  4.87s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:25<01:54,  4.77s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:29<01:41,  4.42s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:33<01:33,  4.26s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:37<01:32,  4.39s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:41<01:22,  4.11s/it]\u001b[A\n",
            "Batch:  65%|██████▌   | 36/55 [02:45<01:16,  4.03s/it]\u001b[A\n",
            "Batch:  67%|██████▋   | 37/55 [02:50<01:18,  4.39s/it]\u001b[A\n",
            "Batch:  69%|██████▉   | 38/55 [02:54<01:11,  4.18s/it]\u001b[A\n",
            "Batch:  71%|███████   | 39/55 [02:57<01:05,  4.07s/it]\u001b[A\n",
            "Batch:  73%|███████▎  | 40/55 [03:03<01:06,  4.42s/it]\u001b[A\n",
            "Batch:  75%|███████▍  | 41/55 [03:07<01:02,  4.44s/it]\u001b[A\n",
            "Batch:  76%|███████▋  | 42/55 [03:11<00:54,  4.18s/it]\u001b[A\n",
            "Batch:  78%|███████▊  | 43/55 [03:15<00:50,  4.23s/it]\u001b[A\n",
            "Batch:  80%|████████  | 44/55 [03:21<00:51,  4.71s/it]\u001b[A\n",
            "Batch:  82%|████████▏ | 45/55 [03:25<00:44,  4.47s/it]\u001b[A\n",
            "Batch:  84%|████████▎ | 46/55 [03:29<00:39,  4.39s/it]\u001b[A\n",
            "Batch:  85%|████████▌ | 47/55 [03:34<00:37,  4.70s/it]\u001b[A\n",
            "Batch:  87%|████████▋ | 48/55 [03:38<00:30,  4.35s/it]\u001b[A\n",
            "Batch:  89%|████████▉ | 49/55 [03:42<00:25,  4.22s/it]\u001b[A\n",
            "Batch:  91%|█████████ | 50/55 [03:48<00:24,  4.85s/it]\u001b[A\n",
            "Batch:  93%|█████████▎| 51/55 [03:53<00:19,  4.81s/it]\u001b[A\n",
            "Batch:  95%|█████████▍| 52/55 [03:56<00:13,  4.38s/it]\u001b[A\n",
            "Batch:  96%|█████████▋| 53/55 [04:01<00:08,  4.50s/it]\u001b[A\n",
            "Batch:  98%|█████████▊| 54/55 [04:05<00:04,  4.30s/it]\u001b[A\n",
            "Batch: 100%|██████████| 55/55 [04:05<00:00,  3.13s/it]\u001b[A\n",
            "Epoch:   1%|▏         | 1/70 [04:05<4:42:46, 245.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Avg Loss: 32811.567525\n",
            "Epoch 1/70 - Loss: 32811.567525 - Train PER: 0.000000 - Val PER: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:04<03:36,  4.01s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:09<04:05,  4.63s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:13<03:44,  4.32s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:16<03:29,  4.11s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:21<03:27,  4.14s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:28<04:15,  5.21s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:33<04:16,  5.34s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:39<04:15,  5.44s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:43<03:45,  4.89s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:48<03:39,  4.87s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:53<03:44,  5.10s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [00:57<03:20,  4.67s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [01:01<03:11,  4.55s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:06<03:05,  4.52s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:10<02:56,  4.42s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:14<02:45,  4.25s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:18<02:40,  4.23s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:23<02:51,  4.65s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:28<02:43,  4.54s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:31<02:27,  4.20s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:37<02:41,  4.76s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:42<02:32,  4.63s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:45<02:19,  4.37s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:50<02:23,  4.61s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [01:54<02:12,  4.41s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [01:59<02:05,  4.34s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:04<02:11,  4.71s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:08<02:00,  4.48s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:12<01:48,  4.19s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:17<01:54,  4.58s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:22<01:48,  4.54s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:25<01:40,  4.35s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:29<01:31,  4.17s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:35<01:40,  4.80s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:40<01:33,  4.66s/it]\u001b[A\n",
            "Batch:  65%|██████▌   | 36/55 [02:44<01:23,  4.42s/it]\u001b[A\n",
            "Batch:  67%|██████▋   | 37/55 [02:49<01:23,  4.66s/it]\u001b[A\n",
            "Batch:  69%|██████▉   | 38/55 [02:53<01:14,  4.40s/it]\u001b[A\n",
            "Batch:  71%|███████   | 39/55 [02:58<01:16,  4.76s/it]\u001b[A\n",
            "Batch:  73%|███████▎  | 40/55 [03:04<01:15,  5.05s/it]\u001b[A\n",
            "Batch:  75%|███████▍  | 41/55 [03:08<01:07,  4.81s/it]\u001b[A\n",
            "Batch:  76%|███████▋  | 42/55 [03:12<00:58,  4.53s/it]\u001b[A\n",
            "Batch:  78%|███████▊  | 43/55 [03:17<00:56,  4.68s/it]\u001b[A\n",
            "Batch:  80%|████████  | 44/55 [03:21<00:49,  4.47s/it]\u001b[A\n",
            "Batch:  82%|████████▏ | 45/55 [03:25<00:41,  4.16s/it]\u001b[A\n",
            "Batch:  84%|████████▎ | 46/55 [03:30<00:39,  4.42s/it]\u001b[A\n",
            "Batch:  85%|████████▌ | 47/55 [03:36<00:39,  4.97s/it]\u001b[A\n",
            "Batch:  87%|████████▋ | 48/55 [03:40<00:32,  4.61s/it]\u001b[A\n",
            "Batch:  89%|████████▉ | 49/55 [03:45<00:29,  4.94s/it]\u001b[A\n",
            "Batch:  91%|█████████ | 50/55 [03:49<00:22,  4.50s/it]\u001b[A\n",
            "Batch:  93%|█████████▎| 51/55 [03:53<00:17,  4.34s/it]\u001b[A\n",
            "Batch:  95%|█████████▍| 52/55 [03:58<00:13,  4.45s/it]\u001b[A\n",
            "Batch:  96%|█████████▋| 53/55 [04:02<00:08,  4.43s/it]\u001b[A\n",
            "Batch:  98%|█████████▊| 54/55 [04:06<00:04,  4.27s/it]\u001b[A\n",
            "Batch: 100%|██████████| 55/55 [04:06<00:00,  3.20s/it]\u001b[A\n",
            "Epoch:   3%|▎         | 2/70 [08:13<4:39:30, 246.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Avg Loss: 31011.472541\n",
            "Epoch 2/70 - Loss: 31011.472541 - Train PER: 0.000000 - Val PER: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:05<04:43,  5.25s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:09<04:16,  4.83s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:13<03:51,  4.45s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:20<04:30,  5.31s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:25<04:23,  5.27s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:31<04:27,  5.47s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:37<04:33,  5.70s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:42<04:11,  5.35s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:47<04:02,  5.28s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:52<03:53,  5.20s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:56<03:38,  4.97s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [01:01<03:24,  4.74s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [01:06<03:27,  4.93s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:10<03:05,  4.52s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:15<03:07,  4.69s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:21<03:22,  5.19s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:25<03:01,  4.77s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:30<03:02,  4.94s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:36<03:12,  5.35s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:40<02:52,  4.94s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:45<02:47,  4.92s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:50<02:43,  4.94s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:54<02:26,  4.58s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:59<02:26,  4.74s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [02:05<02:30,  5.02s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [02:09<02:21,  4.86s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:13<02:08,  4.60s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:19<02:14,  4.97s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:23<01:59,  4.58s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:27<01:51,  4.45s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:32<01:52,  4.69s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:36<01:42,  4.44s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:40<01:34,  4.28s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:44<01:28,  4.20s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:50<01:33,  4.66s/it]\u001b[A\n",
            "Batch:  65%|██████▌   | 36/55 [02:53<01:22,  4.36s/it]\u001b[A\n",
            "Batch:  67%|██████▋   | 37/55 [02:58<01:22,  4.57s/it]\u001b[A\n",
            "Batch:  69%|██████▉   | 38/55 [03:05<01:27,  5.13s/it]\u001b[A\n",
            "Batch:  71%|███████   | 39/55 [03:09<01:15,  4.71s/it]\u001b[A\n",
            "Batch:  73%|███████▎  | 40/55 [03:13<01:10,  4.72s/it]\u001b[A\n",
            "Batch:  75%|███████▍  | 41/55 [03:18<01:08,  4.86s/it]\u001b[A\n",
            "Batch:  76%|███████▋  | 42/55 [03:22<00:59,  4.59s/it]\u001b[A\n",
            "Batch:  78%|███████▊  | 43/55 [03:26<00:52,  4.39s/it]\u001b[A\n",
            "Batch:  80%|████████  | 44/55 [03:32<00:51,  4.65s/it]\u001b[A\n",
            "Batch:  82%|████████▏ | 45/55 [03:35<00:43,  4.36s/it]\u001b[A\n",
            "Batch:  84%|████████▎ | 46/55 [03:40<00:41,  4.58s/it]\u001b[A\n",
            "Batch:  85%|████████▌ | 47/55 [03:47<00:41,  5.14s/it]\u001b[A\n",
            "Batch:  87%|████████▋ | 48/55 [03:51<00:33,  4.75s/it]\u001b[A\n",
            "Batch:  89%|████████▉ | 49/55 [03:55<00:27,  4.62s/it]\u001b[A\n",
            "Batch:  91%|█████████ | 50/55 [04:00<00:24,  4.83s/it]\u001b[A\n",
            "Batch:  93%|█████████▎| 51/55 [04:04<00:18,  4.59s/it]\u001b[A\n",
            "Batch:  95%|█████████▍| 52/55 [04:09<00:13,  4.51s/it]\u001b[A\n",
            "Batch:  96%|█████████▋| 53/55 [04:12<00:08,  4.30s/it]\u001b[A\n",
            "Batch:  98%|█████████▊| 54/55 [04:17<00:04,  4.49s/it]\u001b[A\n",
            "Batch: 100%|██████████| 55/55 [04:18<00:00,  3.33s/it]\u001b[A\n",
            "Epoch:   4%|▍         | 3/70 [12:31<4:41:32, 252.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Avg Loss: 28803.679297\n",
            "Epoch 3/70 - Loss: 28803.679297 - Train PER: 0.000000 - Val PER: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:04<04:27,  4.95s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:09<03:56,  4.47s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:14<04:07,  4.75s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:18<03:54,  4.60s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:22<03:31,  4.23s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:27<03:53,  4.76s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:31<03:33,  4.45s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:35<03:16,  4.18s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:39<03:18,  4.31s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:43<03:09,  4.21s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:47<03:01,  4.12s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [00:52<02:58,  4.16s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [00:57<03:14,  4.63s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:01<02:58,  4.37s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:05<02:48,  4.22s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:11<03:02,  4.69s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:14<02:45,  4.35s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:18<02:32,  4.12s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:24<02:48,  4.69s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:29<02:48,  4.83s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:33<02:33,  4.52s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:39<02:41,  4.90s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:42<02:24,  4.52s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:46<02:16,  4.39s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [01:52<02:23,  4.80s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [01:57<02:23,  4.95s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:02<02:15,  4.83s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:07<02:09,  4.81s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:11<02:04,  4.77s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:15<01:52,  4.49s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:19<01:39,  4.15s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:24<01:44,  4.54s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:28<01:36,  4.37s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:32<01:29,  4.24s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:39<01:41,  5.09s/it]\u001b[A\n",
            "Batch:  65%|██████▌   | 36/55 [02:43<01:32,  4.85s/it]\u001b[A\n",
            "Batch:  67%|██████▋   | 37/55 [02:47<01:21,  4.50s/it]\u001b[A\n",
            "Batch:  69%|██████▉   | 38/55 [02:53<01:21,  4.82s/it]\u001b[A\n",
            "Batch:  71%|███████   | 39/55 [02:57<01:13,  4.61s/it]\u001b[A\n",
            "Batch:  73%|███████▎  | 40/55 [03:01<01:09,  4.63s/it]\u001b[A\n",
            "Batch:  75%|███████▍  | 41/55 [03:07<01:08,  4.87s/it]\u001b[A\n",
            "Batch:  76%|███████▋  | 42/55 [03:10<00:58,  4.52s/it]\u001b[A\n",
            "Batch:  78%|███████▊  | 43/55 [03:14<00:50,  4.25s/it]\u001b[A\n",
            "Batch:  80%|████████  | 44/55 [03:18<00:47,  4.27s/it]\u001b[A\n",
            "Batch:  82%|████████▏ | 45/55 [03:23<00:45,  4.50s/it]\u001b[A\n",
            "Batch:  84%|████████▎ | 46/55 [03:28<00:39,  4.39s/it]\u001b[A\n",
            "Batch:  85%|████████▌ | 47/55 [03:33<00:37,  4.74s/it]\u001b[A\n",
            "Batch:  87%|████████▋ | 48/55 [03:38<00:33,  4.74s/it]\u001b[A\n",
            "Batch:  89%|████████▉ | 49/55 [03:41<00:26,  4.35s/it]\u001b[A\n",
            "Batch:  91%|█████████ | 50/55 [03:45<00:20,  4.16s/it]\u001b[A\n",
            "Batch:  93%|█████████▎| 51/55 [03:50<00:18,  4.53s/it]\u001b[A\n",
            "Batch:  95%|█████████▍| 52/55 [03:54<00:13,  4.39s/it]\u001b[A\n",
            "Batch:  96%|█████████▋| 53/55 [03:59<00:08,  4.31s/it]\u001b[A\n",
            "Batch:  98%|█████████▊| 54/55 [04:05<00:04,  4.90s/it]\u001b[A\n",
            "Batch: 100%|██████████| 55/55 [04:05<00:00,  3.57s/it]\u001b[A\n",
            "Epoch:   6%|▌         | 4/70 [16:37<4:34:42, 249.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Avg Loss: 24790.065230\n",
            "Epoch 4/70 - Loss: 24790.065230 - Train PER: 0.000000 - Val PER: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:05<05:00,  5.56s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:10<04:49,  5.46s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:17<05:09,  5.96s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:21<04:29,  5.28s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:25<03:55,  4.70s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:30<03:57,  4.86s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:34<03:40,  4.60s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:38<03:25,  4.38s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:45<04:00,  5.22s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:49<03:35,  4.79s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:53<03:21,  4.57s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [00:59<03:35,  5.01s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [01:04<03:23,  4.85s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:07<03:03,  4.46s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:12<03:03,  4.59s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:16<02:52,  4.43s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:21<02:51,  4.51s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:26<02:53,  4.68s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:31<02:49,  4.70s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:34<02:34,  4.40s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:39<02:30,  4.42s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:44<02:33,  4.64s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:48<02:21,  4.42s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:53<02:25,  4.69s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [01:57<02:16,  4.56s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [02:02<02:10,  4.48s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:05<01:59,  4.25s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:11<02:02,  4.53s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:15<01:54,  4.41s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:18<01:45,  4.23s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:23<01:46,  4.43s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:27<01:38,  4.26s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:31<01:31,  4.15s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:36<01:30,  4.33s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:42<01:36,  4.83s/it]\u001b[A\n",
            "Batch:  65%|██████▌   | 36/55 [02:46<01:28,  4.66s/it]\u001b[A\n",
            "Batch:  67%|██████▋   | 37/55 [02:50<01:17,  4.33s/it]\u001b[A\n",
            "Batch:  69%|██████▉   | 38/55 [02:55<01:17,  4.59s/it]\u001b[A\n",
            "Batch:  71%|███████   | 39/55 [02:59<01:11,  4.47s/it]\u001b[A\n",
            "Batch:  73%|███████▎  | 40/55 [03:03<01:04,  4.33s/it]\u001b[A\n",
            "Batch:  75%|███████▍  | 41/55 [03:09<01:06,  4.74s/it]\u001b[A\n",
            "Batch:  76%|███████▋  | 42/55 [03:13<00:58,  4.52s/it]\u001b[A\n",
            "Batch:  78%|███████▊  | 43/55 [03:17<00:51,  4.32s/it]\u001b[A\n",
            "Batch:  80%|████████  | 44/55 [03:22<00:49,  4.49s/it]\u001b[A\n",
            "Batch:  82%|████████▏ | 45/55 [03:26<00:44,  4.40s/it]\u001b[A\n",
            "Batch:  84%|████████▎ | 46/55 [03:30<00:38,  4.26s/it]\u001b[A\n",
            "Batch:  85%|████████▌ | 47/55 [03:34<00:34,  4.30s/it]\u001b[A\n",
            "Batch:  87%|████████▋ | 48/55 [03:40<00:33,  4.77s/it]\u001b[A\n",
            "Batch:  89%|████████▉ | 49/55 [03:44<00:26,  4.44s/it]\u001b[A\n",
            "Batch:  91%|█████████ | 50/55 [03:49<00:23,  4.74s/it]\u001b[A\n",
            "Batch:  93%|█████████▎| 51/55 [03:55<00:20,  5.15s/it]\u001b[A\n",
            "Batch:  95%|█████████▍| 52/55 [03:59<00:14,  4.74s/it]\u001b[A\n",
            "Batch:  96%|█████████▋| 53/55 [04:03<00:09,  4.50s/it]\u001b[A\n",
            "Batch:  98%|█████████▊| 54/55 [04:07<00:04,  4.53s/it]\u001b[A\n",
            "Batch: 100%|██████████| 55/55 [04:08<00:00,  3.32s/it]\u001b[A\n",
            "Epoch:   6%|▌         | 4/70 [20:46<4:34:42, 249.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Avg Loss: 19469.408538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   7%|▋         | 5/70 [22:57<5:21:10, 296.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/70 - Loss: 19469.408538 - Train PER: 1.000000 - Val PER: 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Batch:   0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "Batch:   2%|▏         | 1/55 [00:05<04:36,  5.11s/it]\u001b[A\n",
            "Batch:   4%|▎         | 2/55 [00:10<04:29,  5.09s/it]\u001b[A\n",
            "Batch:   5%|▌         | 3/55 [00:14<03:55,  4.54s/it]\u001b[A\n",
            "Batch:   7%|▋         | 4/55 [00:18<03:41,  4.34s/it]\u001b[A\n",
            "Batch:   9%|▉         | 5/55 [00:23<03:55,  4.71s/it]\u001b[A\n",
            "Batch:  11%|█         | 6/55 [00:29<04:09,  5.08s/it]\u001b[A\n",
            "Batch:  13%|█▎        | 7/55 [00:33<03:46,  4.71s/it]\u001b[A\n",
            "Batch:  15%|█▍        | 8/55 [00:38<03:50,  4.90s/it]\u001b[A\n",
            "Batch:  16%|█▋        | 9/55 [00:42<03:38,  4.76s/it]\u001b[A\n",
            "Batch:  18%|█▊        | 10/55 [00:46<03:17,  4.40s/it]\u001b[A\n",
            "Batch:  20%|██        | 11/55 [00:51<03:21,  4.57s/it]\u001b[A\n",
            "Batch:  22%|██▏       | 12/55 [00:56<03:21,  4.69s/it]\u001b[A\n",
            "Batch:  24%|██▎       | 13/55 [01:00<03:04,  4.40s/it]\u001b[A\n",
            "Batch:  25%|██▌       | 14/55 [01:04<03:04,  4.50s/it]\u001b[A\n",
            "Batch:  27%|██▋       | 15/55 [01:10<03:06,  4.66s/it]\u001b[A\n",
            "Batch:  29%|██▉       | 16/55 [01:15<03:08,  4.83s/it]\u001b[A\n",
            "Batch:  31%|███       | 17/55 [01:21<03:17,  5.21s/it]\u001b[A\n",
            "Batch:  33%|███▎      | 18/55 [01:26<03:10,  5.15s/it]\u001b[A\n",
            "Batch:  35%|███▍      | 19/55 [01:29<02:48,  4.69s/it]\u001b[A\n",
            "Batch:  36%|███▋      | 20/55 [01:34<02:47,  4.79s/it]\u001b[A\n",
            "Batch:  38%|███▊      | 21/55 [01:39<02:40,  4.72s/it]\u001b[A\n",
            "Batch:  40%|████      | 22/55 [01:43<02:25,  4.42s/it]\u001b[A\n",
            "Batch:  42%|████▏     | 23/55 [01:47<02:23,  4.48s/it]\u001b[A\n",
            "Batch:  44%|████▎     | 24/55 [01:52<02:21,  4.55s/it]\u001b[A\n",
            "Batch:  45%|████▌     | 25/55 [01:56<02:14,  4.48s/it]\u001b[A\n",
            "Batch:  47%|████▋     | 26/55 [02:01<02:14,  4.65s/it]\u001b[A\n",
            "Batch:  49%|████▉     | 27/55 [02:06<02:11,  4.69s/it]\u001b[A\n",
            "Batch:  51%|█████     | 28/55 [02:10<01:58,  4.40s/it]\u001b[A\n",
            "Batch:  53%|█████▎    | 29/55 [02:14<01:48,  4.19s/it]\u001b[A\n",
            "Batch:  55%|█████▍    | 30/55 [02:19<01:54,  4.59s/it]\u001b[A\n",
            "Batch:  56%|█████▋    | 31/55 [02:23<01:46,  4.45s/it]\u001b[A\n",
            "Batch:  58%|█████▊    | 32/55 [02:27<01:39,  4.34s/it]\u001b[A\n",
            "Batch:  60%|██████    | 33/55 [02:32<01:37,  4.45s/it]\u001b[A\n",
            "Batch:  62%|██████▏   | 34/55 [02:37<01:34,  4.50s/it]\u001b[A\n",
            "Batch:  64%|██████▎   | 35/55 [02:40<01:25,  4.29s/it]\u001b[A\n",
            "Epoch:   7%|▋         | 5/70 [25:40<5:33:40, 308.01s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3026168110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouter_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3036119966.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
          ]
        }
      ],
      "source": [
        "losses, PER_list, PER_list_train = [], [], []\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "outer_loop = tqdm(range(SETTING[\"num_epochs\"]),desc=\"Epoch\", position=0)\n",
        "eval_interval = 5\n",
        "\n",
        "for epoch in outer_loop:\n",
        "  avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
        "  losses.append(avg_loss)\n",
        "\n",
        "  if (epoch + 1) % eval_interval == 0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      PER_val = evaluate_PER(model, dev_loader)\n",
        "      PER_train = evaluate_PER(model, train_loader)\n",
        "      PER_list.append(PER_val)\n",
        "      PER_list_train.append(PER_train)\n",
        "  else:\n",
        "    PER_val = PER_list[-1] if PER_list else 0.0 ## remove if PER_list else 0.0\n",
        "    PER_train = PER_list_train[-1] if PER_list_train else 0.0 ## remove if PER_list_train else 0.0\n",
        "\n",
        "  tqdm.write(f\"Epoch {epoch+1}/{SETTING['num_epochs']} - Loss: {avg_loss:.6f} - Train PER: {PER_train:.6f} - Val PER: {PER_val:.6f}\") ## want to fix this update info at position=0\n",
        "\n",
        "  if early_stopping(PER_val):\n",
        "    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "    break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}