# -*- coding: utf-8 -*-
"""train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wKyRQENL57Cda3WxeeFOJl0edlwea1TA
"""

# need to import conversion_tools.py to use PhonemeASRDataset, EarlyStopping, train_fn, evaluate_PER, early_stopping
# need to import configs.py to use SETTING

if __name__ == "__main__":

  # load data
  train_ds= PhonemeASRDataset(train_feats, train_labels, ipa2idx=ipa2idx)
  dev_ds= PhonemeASRDataset(dev_feats, dev_labels, ipa2idx=ipa2idx)
  # test_ds =  PhonemeASRDataset(test_feats, test_labels, ipa2idx=ipa2idx)

  train_loader = DataLoader(train_ds,
                            batch_size=SETTING["batch_size"],
                            shuffle= True, collate_fn=pad_collate,
                           num_workers=SETTING["num_workers"],
                           pin_memory=SETTING["pin_memory"]) # yields (B, T, C)
  dev_loader = DataLoader(dev_ds,
                        batch_size=SETTING["batch_size"],
                         shuffle=False, collate_fn=pad_collate,
                         num_workers=SETTING["num_workers"],
                        pin_memory=SETTING["pin_memory"])

  #test_loader = DataLoader(test_ds,
                      #batch_size=8,
                      #shuffle=False, collate_fn=pad_collate,
                      #num_workers=SETTING["num_workers"],
                      #pin_memory=SETTING["pin_memory"]))

  early_stopping =  EarlyStopping(patience= SETTING["patience"], delta= 0.001, mode =  "max")

  # instantiate model
  model = ASRModel(
    ip_channel=CONFIG["INPUT_DIM"],
    num_classes=CONFIG["NUM_CLASSES"],
    num_res_blocks=5,
    num_cnn_layers=2,
    cnn_filters=50,
    cnn_kernel_size=15,
    num_rnn_layers=2,
    rnn_dim=170,
    num_dense_layers=1,
    dense_dim=340,
    use_birnn=True,
    rnn_type="lstm",
    rnn_dropout=0.15
).to(SETTING["device"])

  # assign loss function and optimizer
  loss_fn = nn.CTCLoss(blank=0, zero_infinity=True, reduction='sum')  # blank token idx=0
  optimizer = optim.Adam(model.parameters(), lr=SETTING['learning_rate'], weight_decay=SETTING['weight_decay'])

  # train loop
  losses, PER_list, PER_list_train = [], [], []

  torch.autograd.set_detect_anomaly(True)
  outer_loop = tqdm(range(SETTING["num_epochs"]),desc="Epoch", position=0)
  eval_interval = 5

  for epoch in outer_loop:
    avg_loss = train_fn(train_loader, model, optimizer, loss_fn)
    losses.append(avg_loss)
    tqdm.write(f"Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}")

    if (epoch + 1) % eval_interval == 0:
      model.eval()
      with torch.no_grad():
        PER_val = evaluate_PER(model, dev_loader)
        PER_train = evaluate_PER(model, train_loader)
        PER_list.append(PER_val)
        PER_list_train.append(PER_train)
    else:
     PER_val = PER_list[-1] if PER_list else 0.0
      PER_train = PER_list_train[-1] if PER_list_train else 0.0

    tqdm.write(f"Epoch {epoch+1}/{SETTING['num_epochs']} - Loss: {avg_loss:.6f} - Train PER: {PER_train:.6f} - Val PER: {PER_val:.6f}")

    if early_stopping(PER_val):
      print(f"Early stopping triggered at epoch {epoch+1}")
      break

